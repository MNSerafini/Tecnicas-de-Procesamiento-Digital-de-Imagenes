{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Proyecto de OCR"
      ],
      "metadata": {
        "id": "FiBfUujT7pGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importación de Librerias y dataset a utilizar"
      ],
      "metadata": {
        "id": "8a4aNY3G7uyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5pov-j6KuYS",
        "outputId": "1105d0a5-a0a3-4463-e739-66b1bbe1b111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importo librerias a utilizar\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import re\n",
        "import cv2\n",
        "import random\n",
        "from PIL import Image, ImageFilter\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import Image as ImageDisplay\n",
        "from IPython.core.display import HTML\n",
        "from io import BytesIO\n",
        "import base64"
      ],
      "metadata": {
        "id": "4wKf_u1m6yAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importo las librerias\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split # Para dividir los datos"
      ],
      "metadata": {
        "id": "8pAmQw7MKmXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZllGkNL8MwBu",
        "outputId": "5c604322-d39a-462f-fcaf-83b9a0e585db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_YLy0HrekSu"
      },
      "outputs": [],
      "source": [
        "#Convierte imagen en binaria para mostrarla en el dataset\n",
        "def np_to_base64_img(arr):\n",
        "    \"\"\"Convierte un np.array a una imagen embebida en base64\"\"\"\n",
        "    h,w = arr.shape[:2]\n",
        "    im = Image.fromarray(arr)\n",
        "    buffer = BytesIO()\n",
        "    im.save(buffer, format=\"PNG\")\n",
        "    img_str = base64.b64encode(buffer.getvalue()).decode()\n",
        "    return f'<img src=\"data:image/png;base64,{img_str}\" width=\"{w}\" height=\"{h}\"/>'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: quiero convertir esto en un dataset de pandas /content/drive/MyDrive/tp_integrador/letras/character_font.npz y agregar una columna imagen_base64 con el resultado de aplicarle a la imagen en formato np la funcion np_to_base64_img\n",
        "\n",
        "# Cargar los datos del archivo npz\n",
        "data = np.load('/content/drive/MyDrive/tp_integrador/letras/character_font.npz')\n",
        "\n",
        "# Acceder a los arrays dentro del archivo npz\n",
        "# Asumiendo que el archivo .npz contiene las imágenes bajo una clave como 'images' y las etiquetas bajo una clave como 'labels'\n",
        "# Si las claves son diferentes, ajústalas aquí\n",
        "images = data['images']\n",
        "labels = data['labels']\n",
        "\n",
        "# Crear un DataFrame de pandas\n",
        "df_npz = pd.DataFrame({'image': list(images), 'label': list(labels)})\n",
        "\n",
        "# Aplicar la función np_to_base64_img a la columna 'image'\n",
        "df_npz['imagen_base64'] = df_npz['image'].apply(np_to_base64_img)\n",
        "\n",
        "# Mostrar las primeras filas del nuevo DataFrame\n",
        "print(df_npz.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp60eimwA8nF",
        "outputId": "d0a12df8-9355-41aa-e88b-89ab6faf8bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               image  label  \\\n",
            "0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...      0   \n",
            "1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...      1   \n",
            "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...      2   \n",
            "3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...      3   \n",
            "4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...      4   \n",
            "\n",
            "                                       imagen_base64  \n",
            "0  <img src=\"data:image/png;base64,iVBORw0KGgoAAA...  \n",
            "1  <img src=\"data:image/png;base64,iVBORw0KGgoAAA...  \n",
            "2  <img src=\"data:image/png;base64,iVBORw0KGgoAAA...  \n",
            "3  <img src=\"data:image/png;base64,iVBORw0KGgoAAA...  \n",
            "4  <img src=\"data:image/png;base64,iVBORw0KGgoAAA...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: quiero remplazar los labels de df_npz por la letra en el orden correspondiente del alfabeto\n",
        "\n",
        "import string\n",
        "\n",
        "# Crear un mapeo de números a letras del abecedario\n",
        "num_to_letter = {i: char for i, char in enumerate(string.ascii_uppercase)}\n",
        "\n",
        "# Reemplazar las etiquetas numéricas por letras\n",
        "df_npz['label'] = df_npz['label'].apply(lambda x: num_to_letter.get(x, x))\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame con las etiquetas reemplazadas\n",
        "print(df_npz.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKYWBx24IXPy",
        "outputId": "6f16642c-00a5-4b46-ce12-8ff9110839db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               image label  \\\n",
            "0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...     A   \n",
            "1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...     B   \n",
            "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...     C   \n",
            "3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...     D   \n",
            "4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...     E   \n",
            "\n",
            "                                       imagen_base64  \n",
            "0  <img src=\"data:image/png;base64,iVBORw0KGgoAAA...  \n",
            "1  <img src=\"data:image/png;base64,iVBORw0KGgoAAA...  \n",
            "2  <img src=\"data:image/png;base64,iVBORw0KGgoAAA...  \n",
            "3  <img src=\"data:image/png;base64,iVBORw0KGgoAAA...  \n",
            "4  <img src=\"data:image/png;base64,iVBORw0KGgoAAA...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HTML(df_npz[['imagen_base64','label']].sample(20).to_html(escape=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eTwatadqKF8S",
        "outputId": "ef617ad6-0880-4746-e324-8f8c74fb9132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imagen_base64</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>344937</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACGUlEQVR4nNXSS0+TQRTG8f+c931Bm4AGoohFm0YCLRa5iCSFiIpowAWRhXujiTF+DL+IO1ZGFpqIijcEysUggoqCgKnEQKhGKwpt6RwXrZe9G31WZ05+M5nFA/9Fzo+lL4HJHYzkB4SSq5tDTQhjpW5LAMmt1WpeiAl1FbozCG+WTXuVcQBcc2HyinoAOBppTA6mEOzjL/4Dmrvvdjd0lmcNIJkdtbJ2HQRzOyHhoi0BbGNwK9SqAogeivL+uUEwU6vUV1kB0e6gBtry4EhNcjRlENicSkXqEFAvWmQKIjstGOur2b5ww1gENffiJWEU9GCQ5EbliaxBthpqWZwQEGA4TmVZxoGe0lT/030nMQhtkdS4ERDUXZvXUBSDOVU0dG2SJgeQRt/4Q7UgoMQ+hA6j1AaIDcyxu8uSaQ7ycsLNAcvINBGybncxYywsBs4YTEd4YxLRHPAWX+NvRHp8j94yPSDHPfSYb2QcS+6Txr5Yr26VcLX0Lborg+w6KrUVDM+62TywTD4rbradhanRtKfz8eKz2rX/60xaNA+yzlyMir3nTGzZWJbuFJx2OnwjM1jyQN3vs1n/5Xqn95NmJHGX4MUI/UuO/dUZz7S8Sn/QbyHEuIQ/ZlY2ku0U8PMFsszd98r1ySpGLYk+t2zbg3dkfwN1EqNbmuldx6IkblrVW3Gjf/TSIxrX5B4M4OD/rOk6vL/s+j+VHwZA2DXSOMawAAAAAElFTkSuQmCC\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>W</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91887</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACC0lEQVR4nM2Sy0uUYRTGf+/lm29mDM0gHfNGF8RCHEuqIVBsUxgMJFG0iaDAVYsKgqLUhUIhZCCIk5HlKoo2XUAYGpNooMJFuvGymJ1adMGJpikv87ZovpnP/oDo7M77nOec53l44R+XkEoJ4NYgABIQLlTJZcDfsL3pWNHsyufJgYS14hrQq+Bvbq4vFuVVL1/r1Jopjz2RGZ3jr4qWvcEC3y8ZYmhgCrDPBmfn8vtF49XRRLyrrqon87wM22NbcKY3L0AyuNBTD4URE69ACcAnz4+5JGJ5Nai+tYnarHSbjhn/3z5vLE8dyFnmzrR0YdJWXP45F3asS1Xz7pWbbCHa0/OnpMyyNBfTfS5coU8kk+d8UjkvgbfJlnUhh+d/dG8QVrZHXTeP7LwLyaHJzJAP5bTy8MJim9MiJKFx87AI8YciFNsmzDWPg0tFzTPztNrB8VAZNcOlzgKhKXtgxupw7nupeGyiNWjhGNjUb6b24c3igi33TGwPSjqC/B0mFUbnYgvcNeO7cQJBygsmPVqdNSgQu2Im1pjTg0V76lvvpRIUICzBkWlzf4c749aF712txzcjhfZoCjo/Zq4E3H+xNmY6S5tObrS8AG3RlfenPfn7wO1PIwF2DtYDW49GFpeGQ+DJw5ov/SMfZCJ5sNiubNhfEo+8QGeWWV8SCm9+NUtvuoPk3P9H9RtviJA+woqP9wAAAABJRU5ErkJggg==\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187859</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABx0lEQVR4nJWPvWsUQRiHn5ldFY9wwTsFc7pehFioBHOIEPAjEFAwKqbRTkGthfwN+h+IARsLRYR0ljYWEoiFiDZyQgyiYKHR3JFc9vZ2d+a1CE6OmwXxrYb3feb3Af83OgyhevP5HbcJ+6+B5JazU+PRSKmzMAgorXLLwUbj/BmgnniAGILK4as3IuJ4yH5oehZKdpy7PTOkyVZWpr8/+eEB9alrY1EJrD6kVkcuP/byP/goImlmRcSsy7sLnsJMJVFBqADRm28WPnkKT1+vi4iIWJFHEdp1//tYDEIDWQ6QdLAe0Nm7276YW9IIUArc3mWYlbeLz97XT2uBbmyUX/PVfJMgj8tAkmxnc8Ct1V9g2j0EuokWD2jCztzGGUAsBUCIzS1xChCjrQfkoCFOEdhEOQVXZ2t6BqCLK7GtEGAFUoPC9Pq+OMCgQiOZAbaSDloMlySzYgRFq4f4Cldk6QtYi6KV9gFOoXL35cNL1XaCknZhhul9B66f/EyEklYhUFvbqE1OggxkcBZfR48jVhCkOORs+O33sNLKalkrtLh3YmJCARrdSoqA+0cuHq1V95R3lflZaBEsLzN6qnFs/1i4YfqAf84fgjPGFGc1elIAAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>J</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169554</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAAIElEQVR4nGNgIA04HP44A1WEiZCWUQWjCkYVDIgCygEAfqUCxTgtHIMAAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>I</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165826</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABTElEQVR4nOWSyzYDQRCGv+qZkBiRCyLuzsnGgp2N938Gx8KCIMQt5jgkc4kui5hMz1iwV6s+1V/9f3dVwR/itAdSyoks9U4ADHDPga8lRCu73kMGXMV7TVOsxzR34hsAH2BUa8kTotm9UbrNmycyBTMeBC3UUVBW67djkwFKklSWcwEsgYlS5goq09v0sJ59RaBxFPens4rvx72F1XZ1rrDQroehzkyzz/ntzuTSioIgOyvvw4jcAjDTZ6l7CCBqO8FdZIqAiobpxqJFsP6avCdzO38ODLWnA8wnm73+ELFFBSAdvTXaBq+2wssk73sGKIzPzf7ip7flXcROT3IFIYr8GrpdfUwk76uPQ4y0W2P8mrqTLR6D4+7zWajuWJwxK/px/VGNrbir4Vigxg6C9Yq4AhQXBZJoWrgvA6K2mCgDKuXtLVv8iH8C/BpfK8h2xGqrUMsAAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125052</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABaUlEQVR4nNWRu0oDQRSG/5mdyRXRGE0UMZgiah2IdRAC2ka8gJWND2BlbWPhE4j4AHaCjQhaiI0iERG1iCQiMSpEMEFXk92Zschu0N2k17+c+c75OOcA/yJ0VxmysDIWIh2A0SMla/OxLt7+myHmBynsdBaAKojnXu7tYGAAATwfBlMOQtkdGgraUAgE6ndaimJdGPGthbDbzikAhsebNOR0oqTXpd3caFRfy9dnEswEgIk95U41v7+R7QEFQJHcvnqqGW6ovNoPEEATGM8kRyLdPs3WE0I9XGiNtXV7Ns4pJa05abAvlpmNmOxwudDk3fvh0akHqXJZMAAKhBL8pJQQL/sXUQ8ZALMeHA0IQ7Ckeyi1Vu2KkqYMMOsWqg0AgmA8APluKdzxpRdTMD/vwBBeSknqrOe+wYRXsvtLADP5Nptu5nwSYPBXhk2nSArjS3+73TzRBMPpnFdzAnW9UswdHKvmNf9+vgFMM8Aiyn6XjAAAAABJRU5ErkJggg==\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40783</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACR0lEQVR4nI2Ty09TQRSHvzP3trS1lVJoaAyPaCxq0IBg4iPGhTtsdONC125cuTBx4z+iCxduTUzcdGM0gg9i4iMk1kYj+AYEoRShobb0zowLe0sVjZ7VTH7f+Z0zZ2bgHyEbS4Vh4OxAWIuIhM3E1bzrNQEKQ2awNF2yFkSJSg49ugW4vpMhfbSTmXurfkbbvuHc26ZagR0XrhzwN24gGKA1c6nhoAyDI/MXaz7gicVZWcj4gNhA37Gp0Squx/HeEJ/u1pRRWisfcLzYiZk7SwGr02k75wYSpyemsMou+4CVRPLGUqjC9iNudgG6LjuTgqlW6oDoWOpplXVCQ8UsImY2twQWuwYoQOjqyRfFuGcWs2CNynzJYaU9/MJ3IJma9lzPVveql3Zbd8h5NudYr79jrHHMYEtFBD2aGVhna7LyeEZpdnZ9bjSJMWINLFzv3aVf3waMGxvJjyvzE7DwvT6h6QLrABw8PJbH1B1sIvjeAjjGrAGkDkWLD3I11QDSkScGA1pao/GoY6Jh/eYdroffQ487C0BPe0u8sz2ylr/p4RiPBhC0II7nnguMF3Ir1noeaP9i68/BAlZePa+UNvYNQDxjQUvi48QiOGKtaei4QKq8DGIifcWyEqP5JRQw/G0ShHh6bt405W4A/asFhbCle77q/K6jgIhnBEHCteZf0ARoJWBQ5U1qHQiJFXQsNVVmUwUUEBTHDaj9e+6X0JsAF4hu1Rp2p742zacRApyKrBSCHW0fHtq/AJw/ORsPX8v+Qf6P+AFIb+o9DGXLqAAAAABJRU5ErkJggg==\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>351374</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAAj0lEQVR4nGNgIBK8+PV7qy4DMyOGBBMhnaMKRhUMAgVMLEhplwXBZGRiYmL4z8Dw/98/Bsb/eExATfpICr49+/f7779///6pBAr8Z0a3gvE/k2L0I6Z/jP8Z2Kx1pFe+Rjf31bf/SOBFHNxk7N4U91DG4ot/fyFO//P16bUtX7AoYIKa9vfz2XnHsBqMFQAAhuo164m6UvcAAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>L</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56797</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACaklEQVR4nI2Ty2tdVRTGf2vtfc+5jyY1adJac2NTwVfa2oBFB5UIGlFUhEAoFDp1FBD8J9R/wrEifUwS2mkdtKX1AcGRTX3goPFxk5hrTO69e+/l4J5zUke6hpvf/r71rb02/EcJbvbdM62ZbO3K6l5yEQAXeeOjc92R7RtrH3vcxOxMPpKfaFt69GKOx/kaHv/kwriF+sRPX/wtlSy0qOPzBh5tHROcc8+e6v5lYgUQFcFlDRTrbVsM2PhrT8WkpUiIgDmPghkocfKdszAEBIIBuBoKoiKKNWbnDuvQHxjGUYcWkgZyZv4I6FAhJUDUVwBq8enF9iAVBylWCoWtg6k358qQWARwvgJIyfSJ11/OB6IHTYjg0aGrmsErnc37phEoxioHCogFm154cRQRKQHREpDQ2XYOmXnrpaxvAKmy8E5QNldaiz7F5tvpxwfq/qVQ8wDdmyvfB41pcmFpJu2KYBVQ9wBx49bKz9SgvXzpMCpyYJF7AOWXq4+387Rfn1768/o6ZuUAyWoCaK13d/VeX+uWXvjwwiiEIqYn8wLYAPtS46sMyE5edJ9ZrwIaNYC0K7JxeXxsNiPo6SN7m4dMQMDTygVIfUPDtb0P5kjC8fd3p6ITEcHTzIvXs9b+w2uT6bkmQZ6BQdnkoVwZblAIbH/6ye0BkgA3BDyNzAwJgJH1OzfyP863nfUytQIYaYhANIA+2v18vfNeW7xYqXB0rElV4vvhK/lhfv4xTAogs9062jMwsL5KuP3tN52F41UPWw8fjPnRbjjYXvbv/P718vNlip3N37Z2+r3yY1rSWnPvu+u/8j/rH4y75+dv9KncAAAAAElFTkSuQmCC\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310014</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACBklEQVR4nI2Sv2tTURTHv+fe+/IaE19DYwgNRC0IIljRGnTJUASrDrYdXV3FP8BFcPY/sC5OUtRBXUpwKIoU2gjSoVWMtYMNrfoS1NLQ5v44Dqbm5YfoWc/nfs6953uBfxQBQDmfBRMAbjTq6x9ez4eINVuAarPWMWggHcuMFJfm6sJFNOUtZseR2ryVa8n/Arjtu6Il/zOCtpfmw1RwqDAGC5m8WFrYE67jDjvle+GBZObs1KWE1XR0+n1VdQLMTOZruLKQO+8Upc6lqwQAInJXxxaM9Tc/FGmMxNENkCAhfKU+1QiGB72ePTAzO2vIaADEraPREQQiIWV6EJAI93oAEp5A09rRIQthv+x2j2C9owFvqDjma6jGYg2uE5BB7nM6kb1y7QgEY+3xZhfAqcnRhh+L54ehPWzMLtpIXB1ZGMv87s4xkOh+JgA4kORmeeb5d0Ik77bBGcP85FTQ7kXCWn3bGJ4aMEAQ+ylUM+JFeYuZN257yL80lp17mkFsv9deFHk+wvsVOE2F8aTd/1BtQPgJz8wtC5Yue+O4FT0GtlrrWqkipVHjEwddSxENC5B49ghwwNXLYNkHELL+qmKUM6enA3AfACxXZneVQ7w46VvZB3CoPvzILPnwzRO/e50Ag7D24psijTMXAtPHABbmQYkAF7s+wf0AB6wuA4r4ZIHwP/ULyVQFEMNop/UAAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185971</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABdUlEQVR4nO2SvUqcQRSGnzkz386umiKgiGVAAikDXoFNLNJapNLK3lsIeAVpLWxtUolFigi2IiwqKRJdEURxVUSi2e9nZo7FZsOSXaMXkLc6nPNw/uEJmeFe71IZesAqKShATKGSWmYN1gqo+ba1A6CPqrUEDgopbqqEkbEXzigabu8V48ZG5bgDDpzd/njUUfvy3cIM0abdT1+C+sn3yxPOgwO52NwGuGpPzVA27tY+50D7Znqx4UAgHDXJRryv3/6ARLmTN+p+xJ5uXDYsOMjOTqSKiZrV7pBSRRWVvevxGghw2AIFjRFAY0JJklpn+U9wMNtS0L93ldDl0RNwsIUZCINiDgAEvBsSBzSrdZssHrtT9TvDv/UfGACMNQDGSf+v/wHEEm3XSlgzCKRY1l+BkL32IQw/38K+aqXh61y/s1tWjZud92/fCEDR/N5eb1b9T2Iw9RVV1ZB3OoWqpg9Zr1HXg35VIEVeipdgz6/DU+M/Xw8OLbCeJVDcMgAAAABJRU5ErkJggg==\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237202</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAA2ElEQVR4nN2RoU4DQRRFz7x9gS/AQlKD6S9Q1Q/AURQO3b+gkv4ACa6KtLpVCBypxeMq2MXQlO7MQ7CENHmzH7DX3pN3752BbijQv3sta3MU7QIIl6Vn/gNnG9unPKAMT6Jm0oWAcp4C8O0BSRLKsZiFx3Ht3vhCASw8V7mR+rt0dOqZFu4rmFpmg5lZj2ZAjH7JGsllH3SAosjZzYrli1/y4w+YPbRGBAZP/kNto7JLCDfXnp3kaq68SQSO/IACYbUp/POQMIT32zL33QACi8Fk/ZlamA7oB8eInlFgwDrKAAAAAElFTkSuQmCC\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345762</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABGklEQVR4nO2RP0sDQRTE573bJGIjBEUJBtQijTZ+Cb+DpbX4MVIqpBLERrC1shDTiyIIFv6BgJAQmxwkkMIjZ7wdm8vtqatYWDrVMPx23ttd4Nc67ZEk3+73VxC4WDLXHbWHBpTZ6l2jKZzEJgMWm3sdA5ZqG1uV8NYzIjmupm6XDRerczLxh631GQ8ACURFVPWxNV/LUrcDQIIgFFFhzteQcQKxPwFA/vZfABGDqajv20ELRSEkHi+t9Z98QDx4BQCUt5fPvA1mGgBkdXPn5si3DYe9MUAtL1zWz631AC+DWKGj54eTi8T3Wbg66AY0USeEqHuHHBBet9NS0qW5d7BJOgt55RqCogoBkt8AtFY+nv40wq9/4K/0DjMzXjJ4iWFxAAAAAElFTkSuQmCC\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180208</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABJ0lEQVR4nO2TrU4DQRSFz53dDaIhgEP0EZAkGCRvgoeQIHgQFJIHQOAwYHBFgMCUpAkNKQIStoRNy87eOYiZzi7bpDVIPjWZOff/DrCcBBv7N+/KFlO9XgOQArp1sLO5Km07MS+FP3XPSJJaaU1lJxzuBenxUAvnXDvE64kYL7jPnZJ01toqoHbydtQRASDANDMA0OuvQMSIGBH5fLh8Kn0qAAEKLk4HmYNAIIDY/CMkkAIUUHB3a1vF60zgC0zW80Trd+cYPXhYWdcQRMyyPv8L/koQZyFpltRqcm5Ymttf826MG4AA27uD1M3sYcdfM89A8NXrZ4wBxo9Xz2UdjwxLGymL0aFf2iiYW/tRXPtwoVWT5sdpmwaP3zzPFjVK6LqdxT30/ABhuftaIutzqAAAAABJRU5ErkJggg==\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377126</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACHUlEQVR4nI1TS2gTURQ97zMTknZjUxDswg9RUEhBshBR0ZWlRZeKVkERClZBrQjuFEGhKFJduFJwFYLUnfjBTU3BLyLqooV08BtcVKVFpmmbzHvHRTP5TLPo3T3uve+ee865wAricj6wzHfBrb5jiN0rM/DGAEACyBcUuKY3WYYEIGDjmY2SysvVvujzSDuxT0AttaSzJdI/VZ+ROOuTZmS9khIQEoNztAv3N9fyLlKP/xkWhgAXGtg7RsOpHjfEBK3aDryj5astSsMVq7KVgNM3OuCEBUIjPrxoOHM7JaCTxzwajq6FkLUZCsiM0gbFQ1ph+8t5w0/9gKqDFEKj94cls91IXCXpX+xSDR8AQmH1zaKhfwVHv9KUx9NCi0YuhXR0+gkr/HBujBVO9segmgoABzj5ncb/vWBYGnGX5SGBjktkmYZ8uKsJYX3Xrc9nKywHhYPtDQjrUK369uiPphN8fusL2OUFAER0cFOBNOv2dwao6PS2djY3rgjk0pp2bmax5Zo1oj5eGGeFE4ejRIVUz13D8Z80i/kI1VWxLB9k0DbcQqyq3ObXkZjCzjcLUblDw8zeSQnozhNfooYJLfc6rSRc0ZELAk5fb7BcaNrzgAMN9LyImBaJMz5pbm2QVdufLjXbHn0eycn64XTnSqQ/WMvvuUtrvYEkqqfnxHfkA8tnA6FYuzcZiOLTvy4sAELOv5+ywqT6o3q0jP/QgxnD1F6iAwAAAABJRU5ErkJggg==\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142484</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABr0lEQVR4nJWSMWsUQRiGn29m9mJxCsaYWIkgCIrY2ZjSKm36KISIRIRobywkISDif9B/YCeIUdQQUCRNICGJqBA5iUa9cMHbu535LNbb22U3iG81s/vM+77fMPAPCbW7Y17KP0J9Z+45ALP3Dzh7uzstAG+lWhFb/pbFEIGqqmrusKqqypfJqWlghYoGgGP5aLSC69c9PtRz2fmR9hxodjIH4eSS9vToCIJjaVjeZQ7Gn77UircHFLzZjXpdyEUQ6ovXRzqA30CyyrkOhub3ZhvAhv5IOQdoJwkAvjBMX/7cRNRFzc/Xv6oiIB4dTRdjT21SBdS23xwOKLsfCZURbvnaYAzylQMA2vvdGHCaa1kAgiYASe4acoB6O3RmsAPYtVaVg9jWxceHFNQ0ZjasSBFQPq+erw+nmxMOjykCgU/jp9IbVml84NiD+S0pRvjNzX6yvXD5YWkKk70sDfb9elZSjfZSctpzkdUUMMHl3+tf2N5o+ABgeDKT+JISv96ce7Un4Ji/822tVrIgyMuFWAOOzoI7e8WEMrIfuPcCAezNq79NGTBSezbbLn//X/0BMiW+C2d1FbgAAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91651</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAAkElEQVR4nO2TsQ3DMBAD+ULSqPYWRjbQINnB83iHTOHKG3gWuUgA0Y0eIGAJ8QBmRRAn8Yt/4KqGhflblbkMnj+EiW6eEiqwbjDCiFdqADR85uqnZPQ8tCqihFoRQq0IHYClAtRX+KMbuAGVbpT5ylkHKMVNp2I/Gf3BiPfohwPaCQCQ5KCaM+x+k7/YIHs6ACe5KsZRCLr+AAAAAElFTkSuQmCC\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217077</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACbUlEQVR4nI2TTUiUURiFz3vvjN+Yf+mMaeowWkpJJZItBpJcmETQzyLaBOGioDC0aNuiZataREHgqla2iRaKBJlWUqZQZiSWWQ3RNP403/w4Ns58331b3HFQHKizuYv7cN7zHu4F/iECpO07HesPQgqlOBcguOjAkcqZkdeAEPZmBCQge6aCTy7sLQVAlHOO03t5Oj3e44EQuQHA3T7Gs4NXywGXzEHk5QF3FCfGbh8rA0hsIiRcne/YvnU3+P6SZ/M1YNDWvqT62OY7eW9uqAMwskkc+rC5qskIj34KBeairVcaHoVI2htDus4uqOkzRQ4H0Db4qsutk68D9vTG1JtGEgQ4vd39N7fA0HciA9S2u8gMsITIS//oGyjvrlvVjWQykLcO6eAKiDnlEIu9SxcTD2JErB1IoXgHEPoKMAArJa2nz1tPgWVmBAENtYzgdw0AkOZ9Z2c9mLLArhrC79BaaFuIwFDp+QpbZgFfNRBZzDrYih6PnDtIGQcB+LYD4YUsoOAIjhYcrbIyawKVTiAa3tD+50l/k96CAKOEgeVVUPY9MZkT3mZtL0kUOwEksB7glV8lPl2UZSGVT0wpyYI1QeQg26kM7eDxOGKTUULcVhaTEEIQq1WrokXEtUNXbGp+tNFv/gEEkzZQcHUcxpwGri//NNOlsx8Km81oJBOiZv+h4wXPXuqShj0784HITDiyuBQx4zaLQs9ufwve3hhIMgCU+K89nE2keU2KmTmdfHHClSkB5C4qrt67r762cm1JNTM+PPEtCWL9NwGgsHqbu7zMEBJ2Mm7OB74AghkggIgEKQsbJaSt8D/6C3+L9yWAzqvuAAAAAElFTkSuQmCC\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49389</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAB3klEQVR4nK2RPWuTURTHf+fem5IGKyZCS1BEFIqDDh0c6lpcHArOCn4DBfEjSEEEP4CTi+Dg21LQqVWQikOLuPhCtYgxBlHTGPPic+89Dk9SnycNuPif7suP//mfc+AfEjBx/tKpqhs8hE678fHV6qoXiQOA05ePTRfNAFDve+3ml9rKw5agAJRv6Rh9WjqMFQAWHiX9qKqqcaDgk1+q1w9iwMDRvS5FYwghhKhirCspFxcBDOwZ5jPOOedsmiboxOKcwvATFdY3+gj7j886FawkJ+c3wKWNQLQ8uAowdfbKCRXAVA6lJXZUmgL4eXvZCypEyiOAmbDWTUh4X0tNKUgmAyBiwIgWiipAOqYs0NtOIKEyNxMFQPqaByb3NdX6IxfOAKJi+Zp1MHB+wSM6eaCCAZVY38oCAtVqxi6oW3simivhgwJiLaAFGvc3TR5wmUsw29cej3YRAwJiBHBPby43TcwDJp1abH3//O7182ceJb+sN2+TEP3v7o/G5ssaYmMOiJa7S52/bsQwbH9HGjJ5dJg8A5TK39IFqKqOATQmCaLkZaAX06N0e8IuGai3vUKMNDqWUQMMrLWcQwumW9MxDg7qd6Zny/S37q1L2A0ApnjuRbv/4cZMruf/pz9j8NytO/2LhgAAAABJRU5ErkJggg==\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53062</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACG0lEQVR4nNWSPW+ScRTFf/c+vFRrJNS00BcC0loCIiZGNxfj5Bdw0tnEaNRN4+bSxcXByW9gN91NN+MqSkOxESwttBLbkhaKPM//OoDfQc90hntPfvfmwD8h5eqavSMEeBTe2ycUCLH41qqnUVCWUjQRQEkv0gBAmM2x1UdBJJ8+rGKAyuJytzzKlYXCSdlQEMt6jRoGYCmvVcFAzOZDB2VQxI8l2FrHQIaT89YpA4g/mbZuGRRcPm7tpudAXTbJft0zEJfO0FsXFCgmpDkMAZDPSPO3Byjnl6V1JChCMdGry4hxeaFfEweoZDKDrwIK5CKVho0Ys9FvNTPALB3dqZihMJFkcxMHBOFZtse44Tk6nwHFXTlD/Yc4EHd5itamGogV5/hVUUOhdJYGEQMoztC0sIGQS9Hue6BIabq7jQEiF1PHjfFLc9nBBgaKXZJKCwdgRa9WH9slrdcwCDGTpNbxRBEvlqC+44miOp2kWR0NlCaYGAQ4CO6dJ9QPcDj/TsnaG2oAT39a63k6Hovln7Qs2H2RjsdiFx7WzV4SYZyQfHTtUEjcCCMzD0qHwrmbUf/kuwgAH47NNzMzC8z3g5G1nq0V8IAQe2biAsOI7lYLU8PRxad6qxUNALi9N16y7v1bX8yN/ODVrBcal/bN/tCZG3S3HkdZ6fR9M/+ovRLH+1vqyLPa0Prrr4uIp3c/HljQWr0uY8L/Q38AWJ0KYaERlYsAAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>W</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_npz.to_csv('/content/drive/MyDrive/tp_integrador/letras/character.csv', index=False)"
      ],
      "metadata": {
        "id": "CyL1Ykg-DNDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ruta a la carpeta principal que contiene los subdirectorios\n",
        "base_dir = '/content/drive/MyDrive/tp_integrador/numeros'\n",
        "\n",
        "# Lista para almacenar los datos del nuevo dataset\n",
        "data_list = []\n",
        "\n",
        "# Recorrer los subdirectorios\n",
        "for subdir in os.listdir(base_dir):\n",
        "  subdir_path = os.path.join(base_dir, subdir)\n",
        "\n",
        "  # Asegurarse de que sea un directorio\n",
        "  if os.path.isdir(subdir_path):\n",
        "    label = subdir  # El nombre del subdirectorio es la etiqueta\n",
        "\n",
        "    # Recorrer los archivos de imagen dentro del subdirectorio\n",
        "    for filename in os.listdir(subdir_path):\n",
        "      file_path = os.path.join(subdir_path, filename)\n",
        "\n",
        "      # Asegurarse de que sea un archivo y sea una imagen (puedes añadir más extensiones si es necesario)\n",
        "      if os.path.isfile(file_path) and filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
        "        try:\n",
        "          # Leer la imagen en escala de grises\n",
        "          img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
        "          img = 255 - img\n",
        "          img = cv2.resize(img, (32, 32))\n",
        "\n",
        "          if img is not None:\n",
        "            # Convertir la imagen a base64\n",
        "            image_base64 = np_to_base64_img(img)\n",
        "\n",
        "            # Agregar los datos a la lista\n",
        "            data_list.append({'image': img, 'label': label, 'imagen_base64': image_base64})\n",
        "          else:\n",
        "            print(f\"Warning: Could not read image file: {file_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "          print(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "# Crear un DataFrame con los datos de los números\n",
        "df_numeros = pd.DataFrame(data_list)\n",
        "\n",
        "# Unir el nuevo DataFrame con el DataFrame existente df_npz\n",
        "df_combined = pd.concat([df_npz, df_numeros], ignore_index=True)\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame combinado\n",
        "print(\"\\nDataFrame combinado:\")\n",
        "print(df_combined.head())\n",
        "\n",
        "# Puedes mostrar información general sobre el DataFrame combinado\n",
        "print(\"\\nInformación del DataFrame combinado:\")\n",
        "df_combined.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKUnaTysB0-c",
        "outputId": "2ffe1e65-7e60-48bd-c691-d92f4a8b096f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame combinado:\n",
            "                                               image label  \\\n",
            "0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...     A   \n",
            "1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...     B   \n",
            "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...     C   \n",
            "3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...     D   \n",
            "4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...     E   \n",
            "\n",
            "                                       imagen_base64  \n",
            "0  <img src=\"data:image/png;base64,iVBORw0KGgoAAA...  \n",
            "1  <img src=\"data:image/png;base64,iVBORw0KGgoAAA...  \n",
            "2  <img src=\"data:image/png;base64,iVBORw0KGgoAAA...  \n",
            "3  <img src=\"data:image/png;base64,iVBORw0KGgoAAA...  \n",
            "4  <img src=\"data:image/png;base64,iVBORw0KGgoAAA...  \n",
            "\n",
            "Información del DataFrame combinado:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 399925 entries, 0 to 399924\n",
            "Data columns (total 3 columns):\n",
            " #   Column         Non-Null Count   Dtype \n",
            "---  ------         --------------   ----- \n",
            " 0   image          399925 non-null  object\n",
            " 1   label          399925 non-null  object\n",
            " 2   imagen_base64  399925 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 9.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_numeros.to_csv('/content/drive/MyDrive/tp_integrador/numeros/numeros.csv', index=False)"
      ],
      "metadata": {
        "id": "b5JPJu-pDYC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined.to_csv('/content/drive/MyDrive/tp_integrador/combined.csv', index=False)"
      ],
      "metadata": {
        "id": "Slb6DYQQDgH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Leo el dataset por si vuelvo a ejecutar para arrancar desde aca\n",
        "df_combined = pd.read_csv('/content/drive/MyDrive/tp_integrador/combined.csv')"
      ],
      "metadata": {
        "id": "smLpEouhVDO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HTML(df_combined[['imagen_base64','label']].sample(30).to_html(escape=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0S1dctR5D7uF",
        "outputId": "7e926fae-a5ee-4a40-e34e-8152137bbb54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imagen_base64</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>109089</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABLklEQVR4nO2TvUpDQRBGz+y9GgyJRET8QTCghSBipyAES18hra2VD+ALCL6H2FgINikDJoWIYBAjaqPYKaTRGOPdz8KYa0KUdDZ+zS7M2TkMzEI/SR/5F3Wm6cs5AEKAscDC65qayZkR7PW8ORhl5kllYiCbbh7u14L6xOaqs8ftt2Qjs7YxPRYr8lc3ywCje41IVQCGStoCwAFMjZydwAAJB5gjNOqnmowVs6l7hd7MGYCTw/sLm4s71EsV5yUJAEmRt0r50dodCsVLb+qYXNXd51hRADrqgqcDYsVvCT8pqbtgTr4N+B4vFfWp+Af+BDBrHfYDIIUmzOR7Aw6lAmEJsJ6AH15ZdCKTH3//BrSuJltYzy7liAJ/W3w4LtC1xAFB/k7yra+708d4X/kAUEF9MLwRH+kAAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>353317</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABvklEQVR4nI2RvW4TQRSFv7uzu7YEZLMWIUAsO3HHY9DyQqloeQIoKOnoaKkRlSN+hEQFUjA/iZ2EZGWFGNs7cynG68w6isQtRrqrb86ecwb+bww8fFXqXBdjVfeeAhAtmVu5IMGdyQkAsd8EtnqCoH53wumXOiAb90WF0hNlzNnPEABtGVRI/JaCGYSAI8lVbMLoAgBr+DwMgMiynQuO8e4bEQcqFH8DQEi6GRj34/3+SgFVzGZ7DeLx2z/V99RQU0g760DxbrAA3IwQgLSdg6496k1VVCg+9CeRu/xRQvd16dRVTevJ41blf3E2uqYMmh59ny5K9ZiS5TjUzpwvYfCtDli51wAkriyZ47IW097smNpTzg6JQgXydkTM772+Rg5r+DhG68BWTMT+i5eXIlprstUxwOGQK7NoMu8aF+noqKHGAriyDmi+KfNUhgczrAv0Aw9N5inHkyTy3qytA811iDj9NL3Ow50MYi52BvgX0qJAwpi9HCI2n0wQRYXzZ88xZQDsZCAkdytdm1EV64FuBqA+QRm7/tdlEA9sZwBiDIDEthivAA9ugyxVRX8drVS9caMWTcfnV/NeN/8AE/G1KNMFCkIAAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242764</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAB9ElEQVR4nLVTwWpTQRQ9M/PSNKILQbEkjQuLtli7cikFQbRSW90puC8u/RSxoN25qXUhghAUEeqmiiCBSqXF1gZJpUnqpoZqmubNzD0uXvpeYt2JZzMzd87ce8/lDPCvUO1FCfJXLwznjprdHxsrH+fLMMKElgIuzpZq9aYV8WFju1p6OQGYOEsPhmbL7EZlJh8zUph8T9J6LyQp4r31dI/6ddSAwfhnulC6M7TE3oMGAKVPFbnr+SdC7kwCADQeNMKD96TnuzQAqPw6wyQssl/LSfkMEIBT2URQPJhoGx0W6ZL3UXIvJJ0UNQCc3egg8Ono8M1nJMPQcnsCADC+GRNCTucA9F+6/5X8+WTMAAig46o0jVeVjHWbmyuFrKkvlwAgQEPirqgNrEqL3doCgEAECFCNCYqZ27UlCxzJ5k5kfn1ajaSklwZ9IrP4uNTqGxwZGQCeP5yPYi+S2SR6bKvF2mhEuLvDZNLinfNehBSRBQUA6uRa56gTOJYHAA317XVTd7grARlEbeY+cO9vGWStt22Y62U2D95bN92Wn1JTVe65bkuFnm9P64ihderOKknrZN+U1pNvrqBNgFG4tlDrrlCZO4ee2B/KONy6cf7woV5jlPetZuPLTAEp22EgpQTHxy4P9R0z9e/Li4X17p/1X/EbFe/fGR2YbNMAAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140243</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABWElEQVR4nN1Su0oDQRQ9d2Y3DxNDRAxWAX9AMSDYWCmKhaWVlhbmByz8B8XGTiz8AasUImgKuwREq2ARRBELEYyax2Z351hkQ8zkC/RUl5nzmHsY4P9AIblHw2EYNs4ABQDQkl+HWCqD5nk0iosNP7ANQlZjgAMAEowX2HEsA9W97kZzDGtfHEV7XkUOBm8nFIhuNXxFADCSSeqXO8J+2UzWVyCZkvs2IBzsmUjGkZo7aPftq9vpRNxST2zVaTy/G7JZK/YLGMApXJABSZrv4xxkSC0auaOmIcmAvFpwbDFUseZF4U870664sdgvkmC19E6GhiEfdtOWWiCyuT+LUAlA+ag8Z1xDBLp82lKmtyFK9P2RGg/jjvTyOZUP6UaGDAwA0HxeevGgR8ByTg8Se8xQl+sIowCuTI6UQtw8Sp8wtqiN/VekU/EcRoSlLGjdQ9++jh7+ZfwAvnDaF0nKabIAAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76718</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACGklEQVR4nI2SzUuUURSHn/ved+adGU0L8aMyJqEPiRZFWZH0oSQVBC1qYxTRRijQTfQPhOCijf0BRrTJohYubCVj1MKKSlRIUGoqSqUMP0fm473vaaGjeB2o3+YcOM/9Hc65B/4hlU9cn1jd4Sp39uvnwRQosQDXj55v3FdsVJSJ8fHh9yknsKzKWpPyJ9Hd3Z/MiiQaQtru1ZIJnp+MeLHI9puD4reX2/Wjj+XH2eW0pKFHulYBdyWe2Z0dHFHFGRwz378pvhixgdot4VRI0jnANQP3Y8U2UAQ1LqEcEPD7WXwhDzgrcW7R7Kn3lrQLASyMTdvA2xlderUpYnxAlJNN21NUPRA//e46hLWya8s6NyCS/Xj3IDghxUbICTUPpUUyXU2VUKCOQh/vEzG57x3bcAsCcKgjKSI/ey5rnA1fgQrB1juv5kTk9Y2Kgl1woe7pku/L1O1yNlqAA3rnrRERmWgrtQEF4LgQu/ZiVmT4QgEDAOVBfW8mJ53hgnOAAn3xpZEnR9b6AlAWFgUgjjb9vUHgVVvAiTiiFBAYb/6DKMzaaACcrjRJHAMg+LA4aTn4l9q0mIgX9jzYnFNTo5aDqT0wmhhfvoK9zbGxvtWTWlH7G0k/aiyJRL1oTaekWlTeOr/yY1daYWbi26yu2LXj172H09qsB9z9p6ri1UWptOjcl0TfnCOyHgCorSkv87KTn4Yg//4/9BfOussyDfqohAAAAABJRU5ErkJggg==\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379281</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABhklEQVR4nO2Sv2sUURSFv3vfzLiLUTEYs0nAmEII2gTLFGIfgmAXW1tTWVnoH5HWWlELtbGQYCFiSKOwJI2FSSAWQUliAmF/zMw7FruzGAhYiJ2nerzznXvve1z4ez3f1kBfz1PDYeqtpFx6cZWEJ++u3LxWJ3p39fN6y3IE+48/DN+ebC6//9Ersril2NHBgwR8UPlluQTgGNnGTwScaoB6bgq7R8kECU4I1soB8CRU8RCsk1vq4MQyBgNAsayA3inGfk+d8Ljqzk/wjuk/8CfAVIhjX23hd9rS8rDoA7EohWFZAOvbpjp77T5Q7hUG+NnaIB9Fgy8thKPAt0NhpJeGrapfMj72/VPbI46cg+1YWEyuj6i3Mp5Z7UaxsoMLBxGX17LQYXTmTJG7u8VOOXN37Zmq6eH0/ba60uqdjABmydij9YVqYDBj4uGupNi8NwUwvfjq6a1zVHEMdGF+9vJ4Y2ijubV/1A31WvMNSVEBgEcm52anGxdTtLO58vojgcEG/2v9AiiFnbvlGTQ7AAAAAElFTkSuQmCC\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>U</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64329</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABvElEQVR4nIWSzW7TQBSFzx1PnEROSOqkiKa0/O3KghXddcVbsOUVeBJ4A7aAxIJVJdiBhABBqVhUBRXUpCRNm8TFUX7GM5eFnTJxrfRsrsbn0zej8QCXhJL58NGbV+u3ayvL9VrVc4d7L14exYWMx9ZGY9Px6xV/qVrxgFKZfwZDW/TkLXPEs5gpTx435gwgwAEwah222ifBSLof/84Bu0uG+ieDTvP3wWGzM7XcCXA6FPj8dE+p8Xhq1+dAvojg0/YkXjhCEEeGbaCxjOav4gTCAND6osEvIQxZgojB4ItAQYKJdWyw+xlABAhHkjDE2mQYNKDCPjIizocslgHXlTQPJIacwerWFbpWL+gfH3YG8VlsYK2ElQeb3s08EDzrDuCkgVs+cuuUIwCVO9WMLdryOvJQ/V7Q7Xw5BkwaeHf/Lgbt1tGfg939YxDrNNDcQPf1djsc9boRsm4yVFBfn2sAkKSNBST30DuDf9UDAESR3c8MowkKq7Uxi4h57lfMAKUAz0whNVIR/4GKky4tIFJAWWYByUdXMIqLgHtrhJJYsMWNGnCmFhi++afqfZh6bbbheyvs74+zDJfmH3VStnCut1jVAAAAAElFTkSuQmCC\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>F</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285168</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACmElEQVR4nL2RS0iUURzFz73f983DqfFZUVrpmI0SvnKi8kmltrBFj00FgRS2kFZBu3ZRbQTbFWREYVQYWUSYxiQ+CrMm01EJNctRp5EcR9QZnW+c+28xo6Zuo7O4/Ln3xzmH/wX+mWTw0kYfEVlPRkBauecro4hL1vc9+rI5S2aMrXeQoNyiqcr4irFeM5T1AGPK/jb/1TiW0uI9K7EV5/DAJBiOSHeqJ+lHt1KYxMDWAGBkLJCGt2/dIDod+RaBNRkMMvYNzo18rDluNNeqVYC0muDgMRe9ROStSdCdD7TskjlffgqdIr3QMSvgcS8u2AZ2l2nFaoABWUn3XnE8r56Es0V3NHJ5FSFAQB/rqrPPPKh2LSreholsM5Z6cgBgQU164M3IuL1+QkHA96lvU1FskP4CJBZZ4nuNbmveOTMAl23+UCYPm8sAAJLNn518+P3NY0W/Zmxv205k5VhBK4CQIz2TnOaGRVoaHP7W3maLxThLjEIRHGLHnp5+WaPMtHowef+xf/bd1N7DelralQSeVx4BAIi53VUJANGNC3ejQ38aijDlDrkVQaRlc+6dynzA0JqTH+8Jd2AAUivOqBoQyCASF7zfnd86uwpzf3qJC8hgpDX5nvoCErhgUdlJQKb9a3dXcUmHHVwAkBFz+UJsuM+W62416H1oQvnv0XJABjgY06UydxiYaPcqwYbaMfS3x1tkCAYOGRulINNEaDRarYE5PNN115pUDDaxAxl6SQbHvHZbRw+pflX1qyqNtwwOeSKjIjxt9pQyCgTAAVNGsw0QAEhg+oXz9I1Lp/KNU826Yg0AhqrkRNtLq4+BuGAJ+aUFyYuuac8omXL89W0DrQAR0ZM4MAYJOPiMSKWQAkQfruA/6A9gjheYrQ9jcQAAAABJRU5ErkJggg==\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277158</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABz0lEQVR4nJ2Ru2sUURSHvzP3zoTNQxYX1E4xSEQUfICmUixVsBESkBSC/4IIBgtFBFGCf4CGoJaC2KngrkEsUqSIdokoIiKJhCxx3c3O486x2Ed21pigpxvmu7/HObD1RGGiqi4JV6+DEdg1EUaxumr4ELDggzN4mCDfeGELATjTS6EBTOnB41CZW0jegQKVVy45ve9bMZlpmZxdUv1+o9N2x3Q43vGZv1pV9/UkCAABucn4+bb2b9vD7reRaukQFkCCgbHq/Dk832sR4l34qE7HtxsELEdm9Fq/39QD8IT7NXWzo+AJ9N3Vl/sbai0Ajj1VTZ8VBAuXVxdPiUgHIFg7+iPR8i2LMUMldzuXEQAMO+8kqp+Hc+TuaXGvGC8L4LNnNnH1x4OcKS9fREz3QQzBpXnVdGRoWm9aQboBMQw8UKdvSvHro90BWlWHi+rquny+1+8O0NwXYysu/vmkDz+7gtYoc8XIfnkRk24MwFo5JY26rDNVBEzPJoAKaLoJgP4ZfeNC/weIMUZAPJM51PpSNSJ0kMZ1dX+z+BUpaS2btHk10dyJkaD/8KAtv//EwqMlJMt55K9oez4cWJduW6SVlaiWgiZr0WK8Vbd/md8gELsnRdGBMgAAAABJRU5ErkJggg==\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175692</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABuklEQVR4nI3TTUsbURQG4PfczJiR1piQqEQo1Kj4QSKtsXRhoQtxUZAudOVfKQWX/RMuXLqqFCSgUCgKXXRlsS1qkhaaDqa4CJaok8zH68aPmUwGejb3cu7D4XDuvcB/BknSJaufyBa5rt8eaDfr2+JCAupyp9SuvdZK1Y9eqMTsjkv7y1Ng9fPmhC+vbjc/9zw0Ds4wvnzw7gQSAjHrHLCbDkafV74yFgauFgeU5iRm/lagwQsBeB4gzvmzF1uH4vpaVPCF0J2f3a3FyAhg9zzJlKuO+HMB0NRXMvvtwPndoAAAVnxxyCSlOxBBqpBLTggiAIDBrCe51AX9uUAPSlfom1btCEC0GjZ6C2mIdAVA49sf50F+IKIHAs2jel92Jk2R+zYCFTT76BemB/2pIIhLuUZjWAfDt3lT4fup8PEQo4DuHJuQsUl0eTAAAMFl3cLUuEQBSrxu8tEYo4FuHraMEf/874BSAFTP7x82MqPKlU4Qc1qA56iLY+HDvE7VCVyjH9ATBsqWpIupLpPMvVRIzg3DNJFdzCL0s968/0fS+rBcXG/wauuV0QlIkja5vbRW8ciN/k4QGddBKaqera8RmAAAAABJRU5ErkJggg==\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267287</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAChElEQVR4nJXSS09aQRgG4HcGPJzDHQICUi6CUkRLTb3UmPQSb4lpUhdN/4M/q/+gXbRpu2i6cNHYpE1TTauIiEpUBPEAItcz0wUehNYm7bf5MnmfuWRmgP8q37Jf95couhQlCK9qyPBvAW23aWfDxk3BFwrPem9cYF5AYMUtAMCI4SYQw4NIQINgmFAhrO9OCABA6vOwWoaTOeEtIIhl/gcIZWKfFWhDBtq/tUc1pEFZzw72IdzRzdjmiOB0rFCAmqHpAYJlQpx3xgdsAF0AJJjuqYICAG08PqtZcomGDdCd++ILtMKWFdoBhEV+pmnBaWtJo2PO4ohcZ3zbvsg6gE/KCTBz3C9zuWg11f1bUV59NzJIAEALwHl+GjK1qifJ8a0aSuNpG2+M3nr/amJPBa5NxL5pvhSGq8KgktgIfhV1/Uw6gCt7tcVR5H4is18wTG9PFUumevm21n+WjKPiUVcoWA0bokkZ++g7VhR7MzXbrPOabbJ6oAKSatxNmZzxbXcmZJeT0bo1M9A0N39cdq6aBvXnHtlpTemPtFGFNU5b4+nBNy3C1YtibLNeLH/KV62GZo6UzlLG9Pf1dq5+GBrYCWg0FOFnmjVBosamlbVzFTCKqgKs23ZkvUFiNZFdcnQDMAcHED35sBdxSGD9pRZ6wf6QDM68u7NIFiRcpDsPrb3q+TxAxdJxxHx54UX6eqYKQBkcRpcjv5QmuuuvpkJGdUyk+vrr0/JRjtuN0Eq8B3h8EfOU+6DyhFQrh33xxdGH4w7SBci8e3N132iRLIGZ2Jhx/eXTUHYx3BYEAOx1uHe95ZL1kVg6zOZAuIDG87WT7rMAAAwTx4J+R74aubL4l/oFPeL0gdRqSoUAAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>H</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261948</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABl0lEQVR4nI2Sv04bQRCHv9nzH9nHESzFQgkQEFg0iIJESirqNClTpI1Excsg8RQ8AAVSniCthUiAyAl2JCyMEtvEMsZ3OxS276xbiJhmVzOffr+Z2YUnhmFXv7wBDwyy/P7TpJAZn8LJYbUDCoJUirU0AMcHtUtQMJFsN44S6UnMlLo9gwKYFb+fBkQKSzmQkeyzPI6C5uYKo7L1FgY9SQNWgk5fASQKNv801AHmX/xqo4oIpfVmw7VYWfzWGg/8vPL7wrGg7J/9E0RECBabf+N8vIe8XmezIYiRXL6NidJAsTRgOLredO9ihwRY+3D6096ZvPHkXb2r8RRx7KkOw9tBGIX2/HOAN8lPtMzbrezt1sfW0ama5temSlrCACzv73ipfNyDFYPOt6riqeh0B8lbWMWfvQrHD+pOgah6AaL2Ae+RBDDAGS9WEKVYag/T9URBoFKu9x2JaeB1+XvPATJTwEbhh9tEAoiu96/JhI9ZWPX9KMLZZPyrI1Y7dbCPAsy8uqr9FwhenldxF5VsMuu3LtPVp8Q9kDiNCxp/jc4AAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249784</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACQklEQVR4nK2Tz0tUURTHP/e+N89pzHEGJX80hOGMiVGGQpRQEhiGGLSKkrCg/oCINtF/EETbKKqFKS0iCIpwEaEkhkQUgdmoYEZBjGVTOjnz3runxcyoA7qqszlczufy/Z5zOPCvofLJ9oUjR/ckIiH5MTv18nmKqlPdY3dTYAMo23V6Optqo1VhCDe0Hjj+enRHX2xuC3lAiZs4ca4FYObzkglF4+19w6EO6oKrKvV3XPFcf/FJf4PG3nnm4XcR3wzGC/pUD2ZEjMxfKfwJbjubFCNDeUBbNZeXJed5i9dqQFm2BZRdeOOv3GsEwKI3adysyIMWHFuBUrZD5VWRx02AxgQPJVAa//6kk/MERLyckx6ezPx08x46RkRc444046wbT6CxJRbIt7lvr9GolfG0NquAKHe22ALbIz7Cn+Rvo9YmLDqgfE/AhkqMJbhfMsi6HZhsPmsIIQhe2pQAxdDgCYDaqJoHMkoDOrgpkEajsKIOamPg01dQlMUrVCmQf2p490ED5Qe3YpXURWtAoz5OgBJnf0xK6/gG0FhLozNiecQ6I96aC4UVK6cgEr4osiwyfxrt2EqBshyb+qGBtqLPuoGs5FwZ6wLQtqWAphsilwqApvFRRiQnb7urywCwI223xZ06r4p67Lol4nleauBkIoiq67r+PiuveoNW4S6UEO/pbwfMzLdfrlNRm4CbQ+OekoJtrXyOHU7URuujACxMT088nSPgrl4Wlhiau1t3V4dDy4sLyRfPUtj+Jvv73/EX/QDhMmmo5h8AAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>351754</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACIklEQVR4nI2RzUtUYRjFz/O8d+Y6kqITSgXZojZhtNEQ+qIMCYQkimwTbdpUi9ZB/QNtpFW5cFfYJiKiVhJKBhVMigoFfS0s+2BCidGr9+N9T4sZJ+eOi87qhef3nvN8AP+j4wUm/KewODd6sQGmWtcri7QbABv+WZh+2AOpEiMMwoqiOHFlbPJQ1aP9FWvknEuigHdzCsAD0GOXI0OCEFGTzQhgjNjOvVMV4PuQX1wprYQW2eb2XYcH2kCB6o7dU5WI3MaJGnf2zdE6Wi5cQsVhdU/v9sS43MxjOhsEX9/tUydwa8H6J7mwRJJ8Cqj62jxG5xhzcr8AUAAMfiZhspIsAUSo/V1IBGrfzEolAmI8MdYzgAIHb7XSI3XsUXmLNTsXa8/d6YBQ9MvtQobrDutlsHXwaqcTSFgYHo9MPdB47XpDkBNr4hf34YfpCIf8EZc0CgS57i4TShoQ/Dh/YwEWSnPs3tEN7mc/MOEaRwFsGXhOa0my0AeT6gGQ7PKTUnwSTp3ruvxpvm5MRhkzfnPGOqrGvYO2DgBiamFo3rNQ13Ji6yYAHPHgNQgotvVvBgAaT//2CEXTgVSTmoUQiLHohAD8thqAWI3Kr/buFgIAa1ctaMpDnfodp89krQEQfasBfJz6CABifB8GSLA4meohm6/G0Vk17ydSgEvKWWpE1MPEcKl+igoYLZd+zY689ZL0Lcqyq8XPU89eIhNvUqzTX8bEIIEPtOvwAAAAAElFTkSuQmCC\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132844</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACW0lEQVR4nIVSS0+TQRQ99860RVvKo8UHPlojoEZMENSVqHEhxkV1wUa3JoaVOxeuTDRs+A3uXbozcWN04yMxBg2kjRGFWDVQAqVp+Zivnbkuvq9IgcSzmcydk3vOnHuB/4CCI5ZqdxIpLYMEQHy/AgBaX/IDAmnWSggDp59XlEVS+wSQ4ERnsaABQO/1agAw8OB1RTvu9laCvsnRckGDpCtdskwSqUfbGXVk4DEAEtSQAYOQPe8JS2jIJfr9GgcdFFloMKLWwAEGf+qaj6Rfbv5gfjUKjQaqwxdECNwYSVRd//2CaGGtSVymPQ+NDt3ZO85WQcR8Stl9fdfh2K8ZImu/TIMwFlVOmqkwxAEC4umFptDEUGdch4h1dSejKqY4V5gKnzUiq+VNVw0DAJZvTKaKbU7IidWwMcVua/psB+8devjUkgQdSCxsy3TcmYOPnzndCOi7TTBu3lcgTQ/bIYIPqYHPHgLhnQQ4zFQHR4oLAna7EgA39yOXtis1t8MDBfsjcC9mzl2JQm3vIAiUSfx5rydXmGkhkOhEpOTYAQLV+FXqOCmzWyUYxybuXk4EoVlS/ht/7GwrYXgyN34xXGRxwFLydquHeOXR6vHsTxdGT47bwJDQOgC7+Pbj+lUrYUGU88BQmwVgT4a/yVCbbTJIQYMjzdgB2XD5lTt+nv9NT6PW+7sSJSFEWEOAcr63vKwBEkQYYLxKXNK+8Y2pmiITYvV32VO+MWbDmLk1gsZ37r+WJgCqMeo3wG5t9vCtDoBE+o4uAtDomZIQswcQg8LNYnj/+mS3WbbiL+SlBzTCSbJtAAAAAElFTkSuQmCC\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>331460</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACw0lEQVR4nIWTy2sTURTGv3PnTpKZSXxirDZq0aqhVVR84AuqdOELBQUf4Ma/w53+Ca7EnQs3SkXdVJCKoC2CFRVrW4uKaBurqUljMnfSmXvnuOhjuhHP9v64v+87cID/DAG4vm1HaSRoOeJdvQGpifdc2/5qvFDse3YfgARA1ZGJKWs4al/aNhETcQR//Jv7UzHmADMx/FFkx3jf7os3a0LEdZSfTgaYjuYBJX+/q9vNoNR94k5NCPjW9PNAbI19ABAAGpwOpst1/aOc25AGoym5ElRTtAjIZoAUMDB0aJPWMFZawstyY17hs0O0xkybvuWny8MMAES2h4UffGSI8/u369JYocMmQBvB5CQZ6uyl4Jw7n0ZpbEnRBZrGgXBmFQKAz66N9MGTG1F6aJ9abkFpF+QiTBSugK1azqycesLHVxs0oizIQZy08Ah2/+S5w5io5teD6lEWwmUkgEOQ40O14qb42WhXe71Cmdl+c4BiRyC1bOR27mzuce+Bk2KGBJhoATBwLaTdz72ZY2v1QNxRXBFHICQKCGKkhKq88rtav96TVzr/zMw+zAGSAk1W5MgHvce71a3v53dGIWiRwiMVcyhyqZmh1g5uDPqt6zTEIkWOAoYvXbZ+vfc6nXd3TbsGdKLw4MfwpcvmV4+52DLZ88mEMEGicClgKOkA/kDUndef+gYjRMFsCAkgS0EMZXsEXfuBDW/4UTpApGCHc0COFMOXHgF4uXbnzxf9GQWtyAvnFQiYAtsFpPzQt/eoaVRiMgrZJKQy3JAewaLaqNvuQTJrRQlgNTX+CJcAxu/+qGsVGKYpvIU9RNUUkZQAYhH2VC/nw4wVVoyXKMSWtm1LmQEWzcHKgU539+Y1Brn5mtS2q7yrsPG1ADNgBg9e2CtqheLLLwun17LjW2H9qogAMKyvby/tHwq35Efp3ze/aP4C7j5EL1cRIwwAAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313537</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACA0lEQVR4nIVTu24TQRQ9d/bh3ZhNEMiy7MQkvARBjoCKDgEl4iv4AETFD4DoiUTLDyC61DwKiiBAdAkmBkyMhUXsBD9iNt6dQ7Fe71q2xK1mNOeex70a4D8lqaMSU8UXCgcJQAg3Z7U6eqJ3zaz0AXN0y19rdrS5mleMOI3QvP92fQwQHi+835PlR3e0jlQIZbw8wJhBbNfnfPnL410aOvKA/jsmAGrDFm/t9cZkAgKxbaVB+5QtylDjEiICCJnNNn0RRzOdG0gklue3u8QwFI3JrBEDcGG+0lNeu0dwBoMAxWCPxVK1iakaSdhCLC7tNAQCCEUxTEkA0CJYOfv9gNSkpg7idygAGm7YJzyvDWuWhNA9361D3myFnmQAqG7p+o9NH8IYsHC5vQU65VtaAMDYv3hzfVMSBnjnXjXg3r7b8Q0AwNFJfPNVNBETEGRODGGVe08+HSkCIIygGg/EBATMGGJd/fPCT7tLAITla0rxq2+qyAQFYTxxEwjmcrsdiBkgMEKZ7I8mWSjstBAOKcTUKqAAnClW9tWxbmd6UyNAIV87XDhdr83oj1I47l/kV6sNJ0mgAyYAIhRk7Y+H0+0RwGGLsHJLP43xChkMUwyXwiqQv/dgEEcMzdbzZzWJHJnAjd8fQmw/LYGjf0hnLuOmJFZ+1TU+P0wLL14ZzDQ0q/4BuNTSfiPda6gAAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62469</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABlUlEQVR4nIXSyytEURwH8O/vnDtDnjNZUaRYaChJmfKqKY8NO2pQdvIHKJJiq+yUQmps2Ck7KQ1hYTHIK/JYa5rJIyUz5p6fxbjMHfdev+U5n/s9v7pf4J8hAIgVu5gAsPqMP0Y295JSsUnFkqzYmOhRqAeQxp32y5ji4Utvqc/vb6pZepP6nwSdLzqBko65M+bxIggLcDOkSQJa95M8qmkW4DqYXrxqm696KL1GZlD6gCQ/jJ36et3KGjDrUlyuPHXVszUAoNi1cVwZgC1g1qMRrrUHAONcL3MCQPQlzxmoRMoRUCHHHAFXqVMnIOAXYXtAAnWNZyfCDpBU+uT7wodlAhEJpDDQsr4l0/9CM90zg4H89pm1kPGlGQAA8ib6lkPPRqNMwO31lNcHPA9T4Vco41HAaDVT7P4pcUCunRP81tqU8Olu9sbHX5GTymwsMkob7L7nxWqJ7PnpZL9oOOTdNghhDW4GgZpVvh0B3MIyYUAQKmbfE9Oe7/WzQRCUCzkc+ZhvKiCyBCBNIHDHGz7bLmXNF2U63Pj7OjgBAAAAAElFTkSuQmCC\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>384091</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAA50lEQVR4nO2QP0oDURCHv3nsnwi7jSLBCwhiIQSL3EJQT+ABtMsFcodYCMEiepG0sr2VjWChRlCRPHfzfhZb+MQidjYZGPgV38w3DPypBlPVkx4YgEFvUms6AMAtm10BK+D/AOd+J0gAkEA+OAuAU/ACFG8wrCzArO2itJ+KslNb/3AzNEEKTVg/6FudF5Gi2k/pHvlK/iFsrdnecRf3WEWn2umbJEn34/O7Nr2fWaTQ9fADgE6W5gDMh1eKFMnThT/Zkez51c82ZO52fDlLmm+gyV5Gu9u5kSaLNEOLm9E8+1z2w7a+ABS/V5J/NaZmAAAAAElFTkSuQmCC\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>U</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230455</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABmklEQVR4nN2SP2tUQRTFfzNvBleRRRaEWBjFIiColYgQDIgaLVMIgmAT/7RiIwh+ARuxNKAgBEGEWKSxEEkQRJAgiIWIhQouaUSyYeWtM2/mWLy3a1T8AjnNLe6599xzuLB5YOC8sjYirn28f3hEKMz4I/2JnMreq4khwTNdVlIclDUGSZLUvwIOwKQdx1oVOLdRN9NzDcGH/eeyJb95+sNmML4zsxdUaHmkcFVR0oXR9J2glPIS9YYi7j6eM6w8btqtfTuzku/PG+EAm05OGodeHiqNhN0+MTtFhg8LTQiuuKfBXyaV9P2asXUInHqr+E8MmqMYnng7VVlVCKG2n2NM0q1tmFrBjL9QaAZTU78+mG7XfYfRzEFg/eEi3JgSmG+X+r3uKkWqFax5piro0y7g9GeFrLUzQHMgYI90FVJ8QmvLVjeXQ876ssd58zvzu2X+qe5Za3Bm8p1ilq57Rgtov1cVtNw2gONmDFK1emBoEcvlDvKDpfUCMMXic5+wYxc7achgpVTQ66NYwDhmFaWqPIH/z/dtTvwCwW0QvhKnHXMAAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264679</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACcUlEQVR4nI2Rz4uXVRTGP+fc9/3OoI3NoE2ziUS+ighazRDG0JA1OYsJA1dCq/wf2vYv5KZAiMBFUkHMwlpIFoUYlSWlFk2LoZKyHxj4Y5ym7/ve87S471fLjZ7NhXs/93nOcw7cQz35gTTQnRXSkQRVYVTffP/7MFD9/CPIIEyXl6MDhF1fevkXAB58rAC55r1T6ixOSid2AiTGXrsmhaRG1w5YVRSUWPtsmZ6UYtfiprYyCLt+8qIoQFRauWgWWe3kc1MEQFR/HLuSAhxQbZ//oAiDJw6NlrYszn+y6h3QWnz1U0Ly8b19WocMX5/4G1GAjRPNpcZJiqcWXAZAOv2Re+56WD/37a8WOPXBmXCHcP789DLepWRky5YazOu5C2okaRCrR7aC/X/gFVOv34hWkhr9PNurbz95r+dQ+/xvypJCsfbmJnrDfxAD8Ggfmp8iO8j45vga+U4DXliRWkmR9ep9lroOvBwmu3/fNpEgm3/38apMpHQb8Fw9Oy0TEPDOaSMg/8elx8TSeitJyvpxjupWyE5Bo4/uHQmA7DeWlgFTffjgWPffYPvRmzEoMzg3beaWbMf6ma2dgsH0oQ1KIHzti/OqwvPYvpGrVgDDH54bRw6Bn303I9wmno6/VIAUsTDXYoBoPjzjlkEPPG45DZvcsLgbHKKKC18OwiCzfZvlYuEa27+7hGr5562zIHffMWtYiel58sXJ1kvoS6eujlQpRTv7TODdsmzP/o2Ng6z6fWmFJoDN87tuDYn+G1IjKUtvj5arza9ckXSsDxWkhUWRyjj6L5kB9A+Mt9VwzfXOKWQFmJkZbieG+nerfwFe/TbMNJEqXQAAAABJRU5ErkJggg==\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330737</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAB9klEQVR4nK2Ty2tTQRjFz8y9TXK9+Kg1XfjCBsEgqMWdSiwuBHHjSirVFioIBSnduOif4NqN4qog+ADFVRVFQUEXolBtLS2WtBVK4gMSQUxz752Z48KbXDPJwoVnMwznN2e+72MG+Ed1AWce/aTRxhiG8w9ONgz3z+JE2ZHTeTeUEkaTvaLXCnC3TNT4l36MWAnq2KinZPS9VIW/1c+4rz5ZAAoHKeWTyTUAws/tqSxbV2y/TW1mhhpbz5NWwm7PSCzMpoXQEIzWk6Mx6UoJZLcF9TAMgxBSCgv4VqdB/3HAEQBoDK0a0tcZaBYnewA35Qi0a6zCMGL5xuBeAE47IvM3a6Qiv1w71bcBgE04yE+tU5HaBC9He9oBAew6P00yItXa8+F0c4JJBHD0yt0VMlLk7OVsWxWiywH6JqY/k/WQy0OdOoEE/HNvFbXinVwHXwgAmfytmjF8M97JhHBSwIGHpFmYSlIBAKQUAtSRk557Boj0xthIujExGCIAoAML8FQEuBKUCHyAv8qtgHvxyIv5xSoA4NAJGFRftwKHzw4UZj4Uv9Y0Nl0oGImlBhDr6nuSpC59LAYkuTpodXm/FL93panJyqWMNcmBsXtLya94PLy52X8T3Ne/f0e3nxKqXl18+g6uagUcGgA7c91OrTxn4u1/0m/ERvn4McuHsAAAAABJRU5ErkJggg==\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199168</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABWElEQVR4nJ2Sv0oDQRDGv7mNZy4YwcBFojbRykLQJC8gWkSwE23sbKzyCjY+geBr2AsipBGEYGMnIgSxiH+jYhDM7n4WEdyTPUGnGnZ+M/PNzgB/MslguXeSF+cp44apNiuzUVxtCr35Co1OX7OzBadEogIWx2GRLbpZLgD0DQW5EpwWSUArCwxNuE9BosUIrFhMSgoAaEAs8kwBKH0EFDCbAtiwDCOCqXXrB5R+H4C9NA3mCopAdhpeC2SvQ1Lzrp4mEjEsBDeHwc/kLw2ZY2oaXsw4H+GwgsooBESh4F+mYKdHQ8OWm+a4ROsWZIBu6Jcwr45IS83rsn+K01p34BjlB+q5GoxAEG/4p2g+BsYQwrBIL4DnWIVC2DeVOJnvMaPts/YL+dSojvnHAJbW9u3DrkoLSwZYfW2vyHDK2VNHOlLFEjT9APBhLs/vD2jxi80tIGXZ/7VPI9h5hQL5UsEAAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>I</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226853</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAAEklEQVR4nGNgGAWjYBSMAuwAAAQgAAGGsz1EAAAAAElFTkSuQmCC\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264164</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAAyElEQVR4nK2TsQ3CMBBF/zkONSULMAG07JBhqFiBHdiFmg2o2CBKhRISf4qIJLYvDhJcZeue7v9vnYGFEgBgomc/t1eKWp5AYVlnKtcDbf483dbdrFPWrHbI1J4ZDsaowJBCGBHOA0gKKPMTiCYtYbEt0HlGz8OJNasDCgblpyAaaCoT5zmwSnhweFxMY+eBFvejdMmYbRkLeB70moqG8xkC6j58LyEiaQkKEysHwLn4DUZAYPeMdvLatwBQ3+rNCOgB/vSzfq83P05RsQM+umYAAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58066</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABSUlEQVR4nIWSsU4CQRBA39ydBDiIhyhBpNDKxMTaxt4v0Mbv8GOsTWz8D42WWKExsSGicMgFNXDhbiyIsiS3MNVk9u2b3cnAihAj3z872t0p6ah/fdn9L3rz872L43I576pfOcxlys4/VFU10cfTYF41DPF7JcmJ6s/VjXHNmad3I08EdZ/brpMJdF7wFBj0k2yD3odOCkzNcxOQVocU8IsWQF9DFAgCC+B0IxShWjXHZwLxECDdqtla0MNVmTo1yTZAb+xCyoaqBRiEAsJ6MbW0GHwCQtC0GcIBikPQyH4kRBEApU0bMJksBZQ4BoRCYDPEMYrg5W1AOvteLmcD/mprKwBzkIuAN9vQdJoNCH4JAZLYYpBGHVDG3xaDHhwmoDp8M9sab2uelFVQ+XrKBtK6P6Lo6PihLZoFELa2C05BereRu7D5y+MXMAB10IATcEQAAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>I</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347854</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACEklEQVR4nIXTz0vUQRjH8ffMfH/sulprlJr9JrHECDRPWYeKfoCXLgX2H3jsEJ26dotu5aFbQXUq8xBEJRmiRZEEEWYlCIpaUm3lbt/Z7zwddiu/61af67x4ZuZ5ZuD/6bwmicw/unjQgFda9WDu1lxqbwfWZ2yEdJhtbQnqOx+/iNDuT5ULIl9FzgCq9dxbEbl9OIUpVwBSIShIATJ5afH8ao4Z72GkHWjQfpANSi4IMobF+zNFGx7ozYoBNEgxikq7xVFkJTRLix6FzL5mKIFkxMX2k9OGzEYVVwMIJh9rFRfTuKpAEftKxOjPv85Qsa6NWWNix5dpzO9rlmMC/LhAttER5obmkUpQiIhQTUfXG5O/d+M7cQIIFAC1q7c3C3f6RyPtEsCHUzsIU40dtTPDD4YnUUICaGhrAZxbyo0PLfhWSALrc+UytX5j26Gu0yfHro9gnCwHDiZeATRPnN3T1Lp14GZOJ4CCtQF+pGYHWlZtq+tp/3C3WNmo2FprLfbplAdb+rpWtloElDbTC7i82t++olFl5L5FaCfpdeUHszI2BhQ1NVUrAJ4GEeLiX4DKhKAVuUJVoMVvqAPffZylEpgA34n7sX1DUfkMPlNSAfIRETrcfGSnR3786htFxbjzAA27e47XwmD/GMkKHpzYREql65vez069Hn1emncCdHcDMv/uycuRcTy37Gv+Iz8B61/atPjfWD8AAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156591</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABoElEQVR4nN2QvUtbYRTGn3NyP5N7DUkqxRpUCBYVtGhRh0JbEMFBHAKC6ODi11roWFyy6d+grWNBUMSthCIoIrjYJnTRpbRqIn5cTcjNTcjroNHkxr/AZzovz+E8z+8FnoPocQzUFzzWeenuwUJULZDgxmyWbC3wdqLLSB9sbIAFRMUdIxKUgWhie3owunAqxJ8pqTpC1m+EsVCf2osf49UHIVHGOru08k5lB9+cGT9JXyuhwlUR0JpzmVyhWBHCI5PQ3oSllz06FFUpG1wepCZe0Yby/wLtqRw5eYeYCQDuq3DJW7eu96bSeqvxn0sAyph3C0xQG9WwEr8YeP2dH+nu48lDHjQni9/qwLFFyO7P9HsJAOnGRz+Nr0XLqQ8yQwpg+AF0mrT1w6gkB8AwM46q2lmW0HAz8343o7graBQKeiADo5HW5GGfuwLDFhCMAs/6jj51LP+SinArGATkwfnP8pj42VTDQKCGqGb5206/vNv8O5HgUs0BQuwo+bUPQ7/3u8FU44MIAPThnaUXT/kEn6PYZkt/ePWA4CZ8NroFYEt5DYiM6K4AAAAASUVORK5CYII=\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126743</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAB/0lEQVR4nIWTPWhUQRDHf7ubO19yekZR79QgehrUzjQiEghYCDYBQTvFysbWUoQUloZYWFiIYCGChZpOVMRgoSiSwo9ARAn5ulyMd+brLu8lu2OxL/Fd5OJ0O/xm/v+ZYeE/oQAOZ4dnAbRSPitOEkDQc0pXvn0amZj6Ha2mmztOdw71/fBA14O9ANXS5M/FsBaGNpVp3dbW3sL1G0ATHK1F04MT2db8sUxSfDnVFQPi0oNXxqFw8lxni1kpzvyanI2CQkfb5hi98FVuEhhtaH8iMnDGZ1vuyRsADeORncFZ5xj9AHefxwordh4v8X7BVPxAahk+2iYnkLJivsRA9fbrt1hQLrOTcF4pBzg7YB7GHrQ2GsBQuC/VPaR8u+27iTs4hfO5rTmCZgRAKKOdN4kIa4Bqw/p9eym0xz2QzcHBtHggLtPJu2V2wIEMKrlQnXwEKcgFGwBGIJuChoAoqS9fD0SiKIcNAWFpHopLSAMAanPIyMIGwGIZNWx1I8CxUKFWxDQAlFa1RUaX1k3R9NdjRBQyslxvYQ1QAohQsmBU4l+sSkj+1ovj5C2SdnbF1ncBQ/5RVb4PiXzex65LfdcOqbqVap2+KBIO9r+cksdX74yJ9G7BJD04s6ni+nvGct3dJ85ip6fndL2E5si7Z/sJQJ9/Wiq/usw/R2sYfwD7Yc2+U5iyuAAAAABJRU5ErkJggg==\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357717</th>\n",
              "      <td><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABuklEQVR4nNWRzWqTURCGn5l8McYupCBqDQ0JQVGUQiNSFBcq4s6LcONa8CK8E1dS3IoLf7C2aBGpSLMIJILWuhBrG/P7nfO6OMbqJfRdnvPOPDPvwMFQxv1dBUlR21cpYhSXg6IUNVjDwbgwkzsAb7cRUFpyGYjhCo7lxxouAMWXn5CJ5nEARP8pDrp4EgPA1ydZMMrX7Q+9/4oMsysVHMC6nzHMyjcdAzmtHg7x0pGQSp7/JAJHF01AtF+vDUezdUUDsJVesVCKas6kEbTzQgC3t5RLitqbA2DxSZQkTbRedjK4PIsBxo+zJ1yxfveWAEI2ejRwgGdSKpkqpAajhyUHmOsq/+97Clg+ZQ7OjTLTtVMYliaktVWM4Fw7jIEIj2uV+WrlTocIGCxUJw7QHidu71463QONJWmsN0s4ZNQ95T5cLUAh+HjK6u8BeEoZ2NsIMYRQW0gvgS+dgsBJh/S4OQDcak0MUEZ7kAwGEK2/aghTdT53IGbfNk3wF8DOmsBC1iAARG18UNw3iN13BhYqZ6yQWrba9o/B1Plu4Jw/l5iefxwd2kfEwvB92qlxGgNi8Ws3jX8g9BujrPavQX7oLAAAAABJRU5ErkJggg==\" width=\"32\" height=\"32\"/></td>\n",
              "      <td>K</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined.to_csv('/content/drive/MyDrive/tp_integrador/combined.csv', sep=';', index=False)"
      ],
      "metadata": {
        "id": "gZG6Xfgbhr8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones de alteracion de imagenes para el entrenamiento"
      ],
      "metadata": {
        "id": "KsBKTYZy9KMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Funciones de ruido\n",
        "# Ruido de Sal\n",
        "def add_salt_and_pepper_noise(image, prob):\n",
        "    \"\"\"Agrega ruido de sal y pimienta a una imagen PIL.\"\"\"\n",
        "    img_array = np.array(image)\n",
        "    output = np.copy(img_array)\n",
        "    black = 0\n",
        "    white = 255\n",
        "    height, width, channels = img_array.shape if len(img_array.shape) == 3 else (img_array.shape[0], img_array.shape[1], 1)\n",
        "\n",
        "    for i in range(height):\n",
        "        for j in range(width):\n",
        "            if random.random() < prob:\n",
        "                if random.random() < 0.5:\n",
        "                    output[i][j] = black\n",
        "                else:\n",
        "                    output[i][j] = white\n",
        "    return output\n",
        "\n",
        "# Desenfoque\n",
        "def add_gaussian_blur(imagen_gris_cv2, radius):\n",
        "    \"\"\"Aplica desenfoque gaussiano a una imagen CV2 en escala de grises.\"\"\"\n",
        "    # cv2.GaussianBlur espera un tamaño de kernel impar y positivo\n",
        "    # Asegurarse de que el tamaño del kernel sea impar y al menos 1\n",
        "    kernel_size = int(radius * 2) + 1\n",
        "    if kernel_size % 2 == 0: # Si es par, hacerlo impar\n",
        "        kernel_size += 1\n",
        "    kernel_size = max(1, kernel_size) # Asegurarse de que sea al menos 1\n",
        "    return cv2.GaussianBlur(imagen_gris_cv2, (kernel_size, kernel_size), 0)\n",
        "\n",
        "def add_motion_blur(imagen_gris_cv2, radius, angle):\n",
        "    \"\"\"Aplica desenfoque de movimiento a una imagen CV2 en escala de grises.\"\"\"\n",
        "    longitud = int(radius)\n",
        "    if longitud == 0:\n",
        "        return imagen_gris_cv2\n",
        "\n",
        "    kernel = np.zeros((longitud, longitud), dtype=np.float32)\n",
        "    center = longitud // 2\n",
        "    # Crear una línea horizontal para simular el movimiento\n",
        "    cv2.line(kernel, (0, center), (longitud - 1, center), 1, 1)\n",
        "\n",
        "    # Rotar el kernel\n",
        "    M = cv2.getRotationMatrix2D((center, center), angle, 1.0)\n",
        "    kernel = cv2.warpAffine(kernel, M, (longitud, longitud), flags=cv2.INTER_LINEAR)\n",
        "    kernel = kernel / (np.sum(kernel) + 1e-6) # Normalizar y evitar división por cero\n",
        "\n",
        "    output = cv2.filter2D(imagen_gris_cv2, -1, kernel)\n",
        "    return output"
      ],
      "metadata": {
        "id": "A2yVcZXm9VrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Funciones de rotacion, escala y perspectiva\n",
        "\n",
        "#Funcion de rotacion\n",
        "def rotar_imagen(imagen, angulo_rotacion_min, angulo_rotacion_max):\n",
        "    img = Image.fromarray(imagen)\n",
        "    # Aplico una rotación aleatoria dentro del rango permitido\n",
        "    angulos_rotacion = [0, 0, random.uniform(-angulo_rotacion_max, angulo_rotacion_max)]\n",
        "    angulo_rotacion = random.choice(angulos_rotacion)\n",
        "    imagen_rotada = img.rotate(angulo_rotacion, resample=Image.Resampling.BILINEAR, expand=True)\n",
        "    imagen = np.array(imagen_rotada)\n",
        "    return imagen\n",
        "\n",
        "# Perspectiva Vertical\n",
        "def aplicar_perspectiva_vertical(imagen, factor, top=True):\n",
        "    \"\"\"\n",
        "    Aplica una transformación de perspectiva vertical a una imagen.\n",
        "\n",
        "    Args:\n",
        "        imagen (np.ndarray): La imagen de entrada en formato numpy array (BGR o escala de grises).\n",
        "        factor (float): Factor de escala para reducir la distancia de las esquinas (entre 0 y 1).\n",
        "        top (bool): Si es True, aplica la perspectiva a las esquinas superiores.\n",
        "                    Si es False, aplica la perspectiva a las esquinas inferiores.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: La imagen con la perspectiva aplicada.\n",
        "    \"\"\"\n",
        "    height, width = imagen.shape[:2]\n",
        "\n",
        "    # Puntos originales (esquinas de la imagen)\n",
        "    pts1 = np.float32([[0, 0], [width - 1, 0], [width - 1, height - 1], [0, height - 1]])\n",
        "\n",
        "    # Puntos de destino después de la transformación\n",
        "    pts2 = np.float32([[0, 0], [width - 1, 0], [width - 1, height - 1], [0, height - 1]])\n",
        "\n",
        "    if 0 <= factor <= 1:\n",
        "        if top:\n",
        "            # Reduce la distancia entre las dos esquinas superiores\n",
        "            new_top_left_x = int(width * (1 - factor) / 2)\n",
        "            new_top_right_x = width - 1 - new_top_left_x\n",
        "\n",
        "            pts2[0] = [new_top_left_x, 0]\n",
        "            pts2[1] = [new_top_right_x, 0]\n",
        "        else:\n",
        "            # Reduce la distancia entre las dos esquinas inferiores\n",
        "            new_bottom_left_x = int(width * (1 - factor) / 2)\n",
        "            new_bottom_right_x = width - 1 - new_bottom_left_x\n",
        "\n",
        "            pts2[2] = [new_bottom_right_x, height - 1]\n",
        "            pts2[3] = [new_bottom_left_x, height - 1]\n",
        "    else:\n",
        "        print(\"Advertencia: El factor de perspectiva debe estar entre 0 y 1. No se aplicó la transformación.\")\n",
        "        return imagen\n",
        "\n",
        "\n",
        "    # Obtener la matriz de transformación de perspectiva\n",
        "    M = cv2.getPerspectiveTransform(pts1, pts2)\n",
        "\n",
        "    # Aplicar la transformación de perspectiva\n",
        "    imagen_con_perspectiva = cv2.warpPerspective(imagen, M, (width, height))\n",
        "\n",
        "    return imagen_con_perspectiva\n",
        "\n",
        "# Perspectiva horizontal\n",
        "def aplicar_perspectiva_horizontal(imagen, factor, left=True):\n",
        "    \"\"\"\n",
        "    Aplica una transformación de perspectiva horizontal a una imagen.\n",
        "\n",
        "    Args:\n",
        "        imagen (np.ndarray): La imagen de entrada en formato numpy array (BGR o escala de grises).\n",
        "        factor (float): Factor de escala para reducir la distancia de las esquinas (entre 0 y 1).\n",
        "        left (bool): Si es True, aplica la perspectiva a las esquinas izquierdas.\n",
        "                     Si es False, aplica la perspectiva a las esquinas derechas.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: La imagen con la perspectiva aplicada.\n",
        "    \"\"\"\n",
        "    height, width = imagen.shape[:2]\n",
        "\n",
        "    # Puntos originales (esquinas de la imagen)\n",
        "    pts1 = np.float32([[0, 0], [width - 1, 0], [width - 1, height - 1], [0, height - 1]])\n",
        "\n",
        "    # Puntos de destino después de la transformación\n",
        "    pts2 = np.float32([[0, 0], [width - 1, 0], [width - 1, height - 1], [0, height - 1]])\n",
        "\n",
        "    if 0 <= factor <= 1:\n",
        "        if left:\n",
        "            # Reduce la distancia entre las dos esquinas izquierdas\n",
        "            new_top_left_y = int(height * (1 - factor) / 2)\n",
        "            new_bottom_left_y = height - 1 - new_top_left_y\n",
        "\n",
        "            pts2[0] = [0, new_top_left_y]\n",
        "            pts2[3] = [0, new_bottom_left_y]\n",
        "        else:\n",
        "            # Reduce la distancia entre las dos esquinas derechas\n",
        "            new_top_right_y = int(height * (1 - factor) / 2)\n",
        "            new_bottom_right_y = height - 1 - new_top_right_y\n",
        "\n",
        "            pts2[1] = [width - 1, new_top_right_y]\n",
        "            pts2[2] = [width - 1, new_bottom_right_y]\n",
        "    else:\n",
        "        print(\"Advertencia: El factor de perspectiva debe estar entre 0 y 1. No se aplicó la transformación.\")\n",
        "        return imagen\n",
        "\n",
        "\n",
        "    # Obtener la matriz de transformación de perspectiva\n",
        "    M = cv2.getPerspectiveTransform(pts1, pts2)\n",
        "\n",
        "    # Aplicar la transformación de perspectiva\n",
        "    imagen_con_perspectiva = cv2.warpPerspective(imagen, M, (width, height))\n",
        "\n",
        "    return imagen_con_perspectiva\n",
        "\n",
        "# Re escalado de Imagen\n",
        "def reescalar_imagen(imagen_gris_cv2, factor, aumento):\n",
        "    \"\"\"Reescala una imagen CV2 en escala de grises.\n",
        "\n",
        "    Args:\n",
        "        imagen_gris_cv2 (np.ndarray): La imagen de entrada en formato numpy array (escala de grises).\n",
        "        factor (float): El factor base de escalado.\n",
        "        aumento (bool): Si es True, el factor de escala será entre 1.0 y 'factor'.\n",
        "                        Si es False, el factor de escala será entre 1/factor y 1.0.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: La imagen reescalada en escala de grises.\n",
        "    \"\"\"\n",
        "    if aumento:\n",
        "        scale_factor = random.uniform(1.0, factor)\n",
        "    else:\n",
        "        scale_factor = random.uniform(1/factor, 1.0)\n",
        "\n",
        "    new_w = int(imagen_gris_cv2.shape[1] * scale_factor)\n",
        "    new_h = int(imagen_gris_cv2.shape[0] * scale_factor)\n",
        "\n",
        "    # Asegurarse de que el nuevo tamaño no sea cero\n",
        "    new_w = max(1, new_w)\n",
        "    new_h = max(1, new_h)\n",
        "\n",
        "    resized_imagen = cv2.resize(imagen_gris_cv2, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
        "    return resized_imagen\n"
      ],
      "metadata": {
        "id": "lTyYLkHJHTMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones de variacion del color\n",
        "def ajustar_brillo(imagen_gris_cv2, valor_brillo):\n",
        "    \"\"\"Ajusta el brillo de una imagen CV2 en escala de grises.\"\"\"\n",
        "    # Usando np.clip con operaciones aritméticas directas de NumPy para mayor robustez.\n",
        "    return np.clip(imagen_gris_cv2.astype(np.int16) + valor_brillo, 0, 255).astype(np.uint8)\n",
        "\n",
        "def ajustar_contraste(imagen_gris, factor_contraste):\n",
        "    # Asegura que el factor de contraste sea positivo\n",
        "    factor_clipeado = max(0.0, factor_contraste)\n",
        "\n",
        "    # Aplica el ajuste de contraste\n",
        "    # Convertir a float para evitar problemas con la multiplicación y luego a uint8\n",
        "    imagen_float = imagen_gris.astype(np.float32)\n",
        "\n",
        "    # Fórmula: alpha * pixel_value + beta (donde beta = 128 * (1 - alpha) para mantener el punto medio)\n",
        "    # Sin embargo, la forma más directa es como la fórmula mencionada: alpha * (P - medio) + medio\n",
        "    # OpenCV's convertScaleAbs hace (alpha * src + beta)\n",
        "    # Para la fórmula de contraste que dimos, beta sería: 128 * (1 - factor_contraste)\n",
        "\n",
        "    imagen_ajustada = cv2.convertScaleAbs(imagen_float, alpha=factor_clipeado, beta=128 * (1 - factor_clipeado))\n",
        "\n",
        "    return imagen_ajustada"
      ],
      "metadata": {
        "id": "Ryzmc2TgOo-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aplicar_transformacion_aleatoria(imagen_cv2):\n",
        "    \"\"\"\n",
        "    Decide si aplicar transformaciones (50% de probabilidad) y, si lo hace,\n",
        "    aplica entre 1 y 3 transformaciones aleatorias.\n",
        "    Todas las transformaciones reciben y devuelven imágenes CV2 en escala de grises.\n",
        "\n",
        "    Args:\n",
        "        imagen_cv2 (np.ndarray): La imagen de entrada en formato numpy array (BGR o escala de grises).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: La imagen transformada o la original, siempre en escala de grises (CV2).\n",
        "    \"\"\"\n",
        "    # Primero, asegurar que la imagen de entrada esté en escala de grises\n",
        "    if len(imagen_cv2.shape) == 3:\n",
        "        imagen_gris_original = cv2.cvtColor(imagen_cv2, cv2.COLOR_BGR2GRAY)\n",
        "    else:\n",
        "        imagen_gris_original = imagen_cv2.copy() # Copia para no modificar la original\n",
        "\n",
        "    # 50% de probabilidad de no aplicar ninguna transformación\n",
        "    if random.random() < 0.5:\n",
        "        return imagen_gris_original # Devuelve la imagen original en escala de grises\n",
        "\n",
        "    # Si llegamos aquí, vamos a aplicar transformaciones\n",
        "    imagen_actual = imagen_gris_original # Empezamos con la versión gris de la imagen\n",
        "\n",
        "    # Lista de transformaciones disponibles (TODAS operan en CV2 gris y devuelven CV2 gris)\n",
        "    transformaciones_disponibles = [\n",
        "        # Las funciones deben ser las versiones que reciben/devuelven CV2 gris\n",
        "        {'func': add_salt_and_pepper_noise, 'params': {'prob': random.uniform(0.005, 0.03)}},\n",
        "        {'func': add_gaussian_blur, 'params': {'radius': random.uniform(0.5, 1.5)}},\n",
        "        {'func': add_motion_blur, 'params': {'radius': random.randint(5, 15), 'angle': random.uniform(0, 360)}},\n",
        "        {'func': rotar_imagen, 'params': {'angulo_rotacion_min': -5, 'angulo_rotacion_max': 5}},\n",
        "        {'func': aplicar_perspectiva_vertical, 'params': {'factor': random.uniform(0.05, 0.2), 'top': random.choice([True, False])}},\n",
        "        {'func': aplicar_perspectiva_horizontal, 'params': {'factor': random.uniform(0.05, 0.2), 'left': random.choice([True, False])}},\n",
        "        {'func': reescalar_imagen, 'params': {'factor': random.uniform(0.8, 1.2), 'aumento': random.choice([True, False])}},\n",
        "        {'func': ajustar_brillo, 'params': {'valor_brillo': random.randint(-30, 30)}},\n",
        "        {'func': ajustar_contraste, 'params': {'factor_contraste': random.uniform(0.8, 1.2)}},\n",
        "    ]\n",
        "\n",
        "    # Elegir entre 1 y 3 transformaciones para aplicar\n",
        "    num_transformaciones = random.randint(1, 3)\n",
        "    # Usamos random.sample para elegir funciones únicas si num_transformaciones es menor que la lista completa\n",
        "    transformaciones_a_aplicar = random.sample(transformaciones_disponibles, num_transformaciones)\n",
        "\n",
        "    for trans_info in transformaciones_a_aplicar:\n",
        "        func = trans_info['func']\n",
        "        params = trans_info['params']\n",
        "\n",
        "        try:\n",
        "            # Todas las funciones ahora reciben y devuelven CV2 en escala de grises\n",
        "            imagen_actual = func(imagen_actual, **params)\n",
        "        except Exception as e:\n",
        "            print(f\"Error al aplicar la transformación '{func.__name__}': {e}. Se mantiene la imagen actual para la siguiente transformación.\")\n",
        "\n",
        "    return imagen_actual\n"
      ],
      "metadata": {
        "id": "2sfCq-rVRlCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementacion de la red neuronal"
      ],
      "metadata": {
        "id": "LSGoWXeCVWOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variables de ajuste"
      ],
      "metadata": {
        "id": "Umd2BgCdV3-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parámetros globales y mapeos de caracteres (reconfirmar si ya los tienes) ---\n",
        "# Asegúrate de que NUM_CLASES_SALIDA sea 37, como habíamos acordado\n",
        "NUM_CLASES_SALIDA = 37 # A-Z (26) + 0-9 (10) + 'blank' (1)\n",
        "\n",
        "# Mapeo de caracteres a índices (0-36)\n",
        "# El espacio ' ' (blank) va al final, como índice 36\n",
        "VOCABULARIO = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 \"\n",
        "CHAR_TO_INDEX = {char: i for i, char in enumerate(VOCABULARIO)}\n",
        "INDEX_TO_CHAR = {i: char for i, char in enumerate(VOCABULARIO)}\n",
        "\n",
        "# --- Tamaño de las imágenes de entrada a la red ---\n",
        "# Esto es crucial. Todas las imágenes deben tener el mismo tamaño para alimentar la red.\n",
        "# Si tus imágenes originales tienen tamaños variados, deberás preprocesarlas a este tamaño.\n",
        "# Ajusta estos valores según el tamaño promedio/deseado de tus imágenes de OCR.\n",
        "IMAGEN_ANCHO = 32\n",
        "IMAGEN_ALTO = 32"
      ],
      "metadata": {
        "id": "3H61iMPOV6w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definicion de la Red Neuronal"
      ],
      "metadata": {
        "id": "FKB7zgxjalwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Definición de la Red Neuronal (tal cual como antes) ---\n",
        "def build_ocr_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Construye el modelo de red neuronal convolucional para OCR\n",
        "    basado en la descripción provista: [1,36,0,1 Ct5,5,16 Mp3,3 Lfys64 Lfx96 Lrx96 Lbx96 Lbx96 O1c37]\n",
        "    \"\"\"\n",
        "    input_img = keras.Input(shape=input_shape, name=\"image\")\n",
        "\n",
        "    # [Ct5,5,16] - Capa Convolucional\n",
        "    x = layers.Conv2D(\n",
        "        filters=16,\n",
        "        kernel_size=(5, 5),\n",
        "        padding=\"same\",\n",
        "        activation=\"relu\",\n",
        "        name=\"conv_1\",\n",
        "    )(input_img)\n",
        "\n",
        "    # [Mp3,3] - Capa de Max-Pooling\n",
        "    x = layers.MaxPooling2D(pool_size=(3, 3), strides=(3, 3), name=\"pool_1\")(x)\n",
        "\n",
        "    # Aplanar los mapas de características antes de las capas densas (Fully Connected)\n",
        "    x = layers.Flatten(name=\"flatten\")(x)\n",
        "\n",
        "    # [Lfys64] - Capa Fully Connected con 64 neuronas\n",
        "    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(x)\n",
        "\n",
        "    # [Lfx96] - Capa Fully Connected con 96 neuronas\n",
        "    x = layers.Dense(96, activation=\"relu\", name=\"dense_2\")(x)\n",
        "\n",
        "    # [Lrx96] - Capa Fully Connected con 96 neuronas\n",
        "    x = layers.Dense(96, activation=\"relu\", name=\"dense_3\")(x)\n",
        "\n",
        "    # [Lbx96] - Capa Fully Connected con 96 neuronas\n",
        "    x = layers.Dense(96, activation=\"relu\", name=\"dense_4\")(x)\n",
        "\n",
        "    # [Lbx96] - Otra Capa Fully Connected con 96 neuronas\n",
        "    x = layers.Dense(96, activation=\"relu\", name=\"dense_5\")(x)\n",
        "\n",
        "    # [O1c37] - Capa de Salida\n",
        "    # La capa de salida debe tener `num_classes` (37) neuronas\n",
        "    # y usar activación softmax para clasificación multiclase.\n",
        "    output = layers.Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n",
        "\n",
        "    model = keras.Model(inputs=input_img, outputs=output, name=\"ocr_model\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "JTOkdJD0ajq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Función de preprocesamiento de imágenes ---\n",
        "def preprocess_image(image_np_array, target_size=(IMAGEN_ANCHO, IMAGEN_ALTO)):\n",
        "    \"\"\"\n",
        "    Normaliza y redimensiona la imagen para la entrada a la red.\n",
        "    Asume que la imagen_np_array ya está en escala de grises.\n",
        "    \"\"\"\n",
        "    # Asegurarse de que la imagen esté en el formato correcto (ALTO, ANCHO)\n",
        "    if image_np_array.shape != (target_size[1], target_size[0]): # Nota: target_size es (ancho, alto)\n",
        "        image_np_array = cv2.resize(image_np_array, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # Normalizar los píxeles a [0, 1]\n",
        "    image_normalized = image_np_array.astype(np.float32) / 255.0\n",
        "\n",
        "    # Keras espera las imágenes con un canal final si son monocromáticas (alto, ancho, 1)\n",
        "    image_final = np.expand_dims(image_normalized, axis=-1)\n",
        "\n",
        "    return image_final\n",
        "\n",
        "# --- Función para codificar etiquetas ---\n",
        "def encode_labels(label_text, char_to_index_map):\n",
        "    \"\"\"\n",
        "    Codifica un string de etiqueta a un índice numérico.\n",
        "    \"\"\"\n",
        "    if label_text in char_to_index_map:\n",
        "        return char_to_index_map[label_text]\n",
        "    else:\n",
        "        print(f\"Advertencia: Caracter '{label_text}' no encontrado en el vocabulario. Mapeando a blank.\")\n",
        "        return char_to_index_map[' '] # Mapea a 'blank'\n",
        "\n",
        "# --- Función para cargar tu dataset (Ejemplo con un DataFrame) ---\n",
        "def cargar_dataset_desde_dataframe(df):\n",
        "    \"\"\"\n",
        "    Carga imágenes y etiquetas desde tu DataFrame.\n",
        "    Esta es una implementación dummy para probar el código.\n",
        "    \"\"\"\n",
        "    dummy_data = []\n",
        "    # Genera algunas imágenes y etiquetas de prueba (A-Z, 0-9)\n",
        "    for char_val in VOCABULARIO[:-1]: # Excluir 'blank' para generación de data\n",
        "        dummy_img = np.zeros((IMAGEN_ALTO, IMAGEN_ANCHO), dtype=np.uint8)\n",
        "        # Dibujar el caracter en la imagen\n",
        "        text_size = cv2.getTextSize(char_val, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)[0]\n",
        "        text_x = (IMAGEN_ANCHO - text_size[0]) // 2\n",
        "        text_y = (IMAGEN_ALTO + text_size[1]) // 2\n",
        "        cv2.putText(dummy_img, char_val, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, (255), 2, cv2.LINE_AA)\n",
        "        dummy_data.append({'image_np': dummy_img, 'label': char_val})\n",
        "\n",
        "    # Multiplicar los datos para tener más volumen si es necesario\n",
        "    dummy_df = pd.DataFrame(dummy_data)\n",
        "    # Aumentar artificialmente el tamaño para pruebas\n",
        "    dummy_df = pd.concat([dummy_df] * 50, ignore_index=True)\n",
        "\n",
        "    print(f\"Dataset cargado. Total de {len(dummy_df)} muestras.\")\n",
        "    return dummy_df\n",
        "\n",
        "\n",
        "# --- Bucle de Entrenamiento por Épocas (Adaptado) ---\n",
        "def entrenar_modelo_ocr(dataframe_datos, num_epochs=10, batch_size=32, train_split=0.8):\n",
        "\n",
        "    train_data = dataframe_datos\n",
        "\n",
        "    # 2. Construir el modelo\n",
        "    input_shape = (IMAGEN_ALTO, IMAGEN_ANCHO, 1) # (alto, ancho, canales)\n",
        "    model = build_ocr_model(input_shape, NUM_CLASES_SALIDA)\n",
        "\n",
        "    # Compilar el modelo\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    print(\"\\nResumen del modelo:\")\n",
        "    model.summary()\n",
        "\n",
        "    # 3. Bucle de entrenamiento\n",
        "    print(f\"\\nComenzando el entrenamiento por {num_epochs} épocas...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\n--- Época {epoch + 1}/{num_epochs} ---\")\n",
        "\n",
        "        # Mezclar el dataframe en cada época para asegurar aleatoriedad\n",
        "        shuffled_train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "        # Almacenará las imágenes y etiquetas procesadas para la época actual\n",
        "        epoch_images = []\n",
        "        epoch_labels = []\n",
        "\n",
        "        # Procesar todas las imágenes de la época, llamando a aplicar_transformacion_aleatoria\n",
        "        # Esta función ahora decide internamente si transformar o no, y cuántas transformaciones aplicar\n",
        "        print(f\"Procesando {len(shuffled_train_data)} muestras para la época (con aumento de datos)...\")\n",
        "        for _, row in shuffled_train_data.iterrows():\n",
        "            img = row['image']\n",
        "            label = row['label']\n",
        "\n",
        "            # Aquí llamamos a la función que ahora encapsula toda la lógica de aumento de datos\n",
        "            transformed_or_original_img = aplicar_transformacion_aleatoria(img)\n",
        "\n",
        "            # Preprocesar la imagen (ya sea transformada u original)\n",
        "            processed_img = preprocess_image(transformed_or_original_img, target_size=(IMAGEN_ANCHO, IMAGEN_ALTO))\n",
        "            encoded_label = encode_labels(label, CHAR_TO_INDEX) # La etiqueta no cambia por las transformaciones visuales\n",
        "\n",
        "            epoch_images.append(processed_img)\n",
        "            epoch_labels.append(encoded_label)\n",
        "\n",
        "        # Convertir a NumPy arrays para el entrenamiento\n",
        "        X_epoch = np.array(epoch_images)\n",
        "        y_epoch = np.array(epoch_labels)\n",
        "\n",
        "        # Mezclar nuevamente el conjunto de datos de la época antes del entrenamiento\n",
        "        indices = np.arange(len(X_epoch))\n",
        "        np.random.shuffle(indices)\n",
        "        X_epoch = X_epoch[indices]\n",
        "        y_epoch = y_epoch[indices]\n",
        "\n",
        "        print(f\"Entrenando con {len(X_epoch)} muestras para esta época...\")\n",
        "\n",
        "        # Entrenar el modelo con los datos de esta época\n",
        "        history = model.fit(\n",
        "            X_epoch,\n",
        "            y_epoch,\n",
        "            batch_size=batch_size,\n",
        "            epochs=1, # Entrenar por 1 época en cada iteración de este bucle\n",
        "            verbose=1,\n",
        "        )\n",
        "\n",
        "    print(\"\\nEntrenamiento completado.\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "Gki6Ct_Bbd46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Bloque principal para ejecutar el entrenamiento ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Simular la carga de tu DataFrame\n",
        "    # REEMPLAZA ESTA LLAMADA con la forma en que cargas tu DataFrame real\n",
        "    df_entrenamiento = df_combined\n",
        "\n",
        "    # Ejecutar el entrenamiento\n",
        "    modelo_entrenado = entrenar_modelo_ocr(\n",
        "        dataframe_datos=df_entrenamiento,\n",
        "        num_epochs=30,     # Número de épocas de entrenamiento\n",
        "        batch_size=64     # Tamaño del lote (batch size)\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uH8nzQsfbk6E",
        "outputId": "21c221a7-2bbd-4cb7-fad8-11b6584dd843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumen del modelo:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"ocr_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ocr_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ image (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_1 (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │           \u001b[38;5;34m416\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ pool_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m102,464\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │         \u001b[38;5;34m6,240\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │         \u001b[38;5;34m9,312\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │         \u001b[38;5;34m9,312\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │         \u001b[38;5;34m9,312\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m)             │         \u001b[38;5;34m3,589\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ image (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ pool_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">102,464</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,240</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,312</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,312</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,312</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,589</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m140,645\u001b[0m (549.39 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">140,645</span> (549.39 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m140,645\u001b[0m (549.39 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">140,645</span> (549.39 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comenzando el entrenamiento por 30 épocas...\n",
            "\n",
            "--- Época 1/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.5774 - loss: 1.5520\n",
            "\n",
            "--- Época 2/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - accuracy: 0.7131 - loss: 1.0289\n",
            "\n",
            "--- Época 3/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7346 - loss: 0.9479\n",
            "\n",
            "--- Época 4/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7444 - loss: 0.9134\n",
            "\n",
            "--- Época 5/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - accuracy: 0.7520 - loss: 0.8828\n",
            "\n",
            "--- Época 6/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7561 - loss: 0.8671\n",
            "\n",
            "--- Época 7/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7605 - loss: 0.8524\n",
            "\n",
            "--- Época 8/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7645 - loss: 0.8418\n",
            "\n",
            "--- Época 9/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7648 - loss: 0.8347\n",
            "\n",
            "--- Época 10/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7687 - loss: 0.8263\n",
            "\n",
            "--- Época 11/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7701 - loss: 0.8187\n",
            "\n",
            "--- Época 12/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7713 - loss: 0.8137\n",
            "\n",
            "--- Época 13/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7719 - loss: 0.8129\n",
            "\n",
            "--- Época 14/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7732 - loss: 0.8054\n",
            "\n",
            "--- Época 15/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7747 - loss: 0.8013\n",
            "\n",
            "--- Época 16/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7749 - loss: 0.8010\n",
            "\n",
            "--- Época 17/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7762 - loss: 0.7941\n",
            "\n",
            "--- Época 18/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7777 - loss: 0.7926\n",
            "\n",
            "--- Época 19/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7798 - loss: 0.7846\n",
            "\n",
            "--- Época 20/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7815 - loss: 0.7781\n",
            "\n",
            "--- Época 21/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7805 - loss: 0.7819\n",
            "\n",
            "--- Época 22/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7801 - loss: 0.7809\n",
            "\n",
            "--- Época 23/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7816 - loss: 0.7769\n",
            "\n",
            "--- Época 24/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7818 - loss: 0.7766\n",
            "\n",
            "--- Época 25/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7833 - loss: 0.7697\n",
            "\n",
            "--- Época 26/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7815 - loss: 0.7768\n",
            "\n",
            "--- Época 27/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7828 - loss: 0.7710\n",
            "\n",
            "--- Época 28/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - accuracy: 0.7830 - loss: 0.7704\n",
            "\n",
            "--- Época 29/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7837 - loss: 0.7697\n",
            "\n",
            "--- Época 30/30 ---\n",
            "Procesando 399925 muestras para la época (con aumento de datos)...\n",
            "Entrenando con 399925 muestras para esta época...\n",
            "\u001b[1m6249/6249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7853 - loss: 0.7638\n",
            "\n",
            "Entrenamiento completado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nombre_del_modelo = \"/content/drive/MyDrive/tp_integrador/ocr_model_final_30_EPOCAS.keras\" # Formato recomendado para Keras 3+\n",
        "\n",
        "# nombre_del_modelo = \"ocr_model_final.h5\" # Formato HDF5, compatible con Keras 2 y versiones anteriores de TF\n",
        "modelo_entrenado.save(nombre_del_modelo)\n",
        "print(f\"Modelo guardado exitosamente como '{nombre_del_modelo}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfMSqaWa3i8s",
        "outputId": "f7590ffc-dfac-40c7-97ab-fe481dbd2a00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo guardado exitosamente como '/content/drive/MyDrive/tp_integrador/ocr_model_final_30_EPOCAS.keras'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prueba del Modelo"
      ],
      "metadata": {
        "id": "5_jhi_hz5t_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model(nombre_del_modelo)\n",
        "import gradio as gr\n",
        "\n",
        "# --- Función de preprocesamiento de imágenes (de tu código de entrenamiento) ---\n",
        "def preprocess_image(image_np_array, target_size=(IMAGEN_ANCHO, IMAGEN_ALTO)):\n",
        "    \"\"\"\n",
        "    Normaliza y redimensiona la imagen para la entrada a la red.\n",
        "    Asume que la imagen_np_array ya está en escala de grises.\n",
        "    \"\"\"\n",
        "    # Asegurarse de que la imagen esté en el formato correcto (ALTO, ANCHO)\n",
        "    if image_np_array.shape != (target_size[1], target_size[0]): # Nota: target_size es (ancho, alto)\n",
        "        image_np_array = cv2.resize(image_np_array, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # Normalizar los píxeles a [0, 1]\n",
        "    image_normalized = image_np_array.astype(np.float32) / 255.0\n",
        "\n",
        "    # Keras espera las imágenes con un canal final si son monocromáticas (alto, ancho, 1)\n",
        "    image_final = np.expand_dims(image_normalized, axis=-1)\n",
        "\n",
        "    return image_final\n",
        "\n",
        "# --- Función principal de inferencia de OCR ---\n",
        "def run_ocr_inference(input_image):\n",
        "    \"\"\"\n",
        "    Procesa una imagen de entrada para OCR: detecta contornos,\n",
        "    separa en cajas, analiza cada caja con el modelo y concatena los resultados.\n",
        "\n",
        "    Args:\n",
        "        input_image (np.ndarray): La imagen de entrada desde Gradio (puede ser RGB).\n",
        "\n",
        "    Returns:\n",
        "        str: El string concatenado de los caracteres reconocidos.\n",
        "        np.ndarray: La imagen con los contornos y bounding boxes dibujados para visualización.\n",
        "    \"\"\"\n",
        "    if model is None:\n",
        "        return \"Error: El modelo de OCR no se pudo cargar.\", input_image\n",
        "\n",
        "    # Convertir la imagen a escala de grises\n",
        "    # Gradio suele pasar imágenes como RGB, así que convertimos a BGR para OpenCV\n",
        "    # y luego a escala de grises.\n",
        "    if len(input_image.shape) == 3 and input_image.shape[2] == 3:\n",
        "        gray_image = cv2.cvtColor(input_image, cv2.COLOR_RGB2GRAY)\n",
        "    else:\n",
        "        gray_image = input_image.copy() # Ya es escala de grises\n",
        "\n",
        "    # Preparar una copia de la imagen para dibujar los contornos\n",
        "    display_image = cv2.cvtColor(gray_image, cv2.COLOR_GRAY2BGR) # Convertir a BGR para dibujar en color\n",
        "\n",
        "    # Binarizar la imagen (umbralización) para facilitar la detección de contornos\n",
        "    # Puedes experimentar con diferentes umbrales o métodos de binarización\n",
        "    # Se mantiene la binarización global para la detección de contornos,\n",
        "    # pero cada ROI se binarizará individualmente a blanco sobre negro.\n",
        "    _, binary_for_contours = cv2.threshold(gray_image, 150, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Encontrar contornos en la imagen binarizada\n",
        "    # cv2.RETR_EXTERNAL para solo contornos externos, cv2.CHAIN_APPROX_SIMPLE para puntos simplificados\n",
        "    contours, _ = cv2.findContours(binary_for_contours, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    detected_characters = []\n",
        "\n",
        "    # Almacenar bounding boxes junto con sus coordenadas para ordenar\n",
        "    boxes = []\n",
        "\n",
        "    # Iterar sobre cada contorno encontrado\n",
        "    for contour in contours:\n",
        "        # Calcular el rectángulo delimitador (bounding box) para cada contorno\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Filtrar contornos muy pequeños o muy grandes que no sean caracteres\n",
        "        # Estos valores son heurísticos y pueden necesitar ajuste para tu dataset\n",
        "        min_area = 50 # Área mínima para considerar un contorno como carácter\n",
        "        min_aspect_ratio = 0.1 # Relación ancho/alto mínima\n",
        "        max_aspect_ratio = 10 # Relación ancho/alto máxima\n",
        "\n",
        "        if w * h < min_area:\n",
        "            continue\n",
        "        if not (min_aspect_ratio < w/h < max_aspect_ratio):\n",
        "            continue\n",
        "\n",
        "        # También puedes filtrar por altura o ancho mínimo/máximo si sabes el rango de tus caracteres\n",
        "        # if h < 10 or w < 5: continue # Ejemplo: altura mínima 10px, ancho mínimo 5px\n",
        "\n",
        "        # Guardar la caja con su coordenada X para el ordenamiento\n",
        "        boxes.append({\"box\": (x, y, w, h), \"x_coord\": x})\n",
        "\n",
        "    # --- Lógica de Ordenamiento Modificada ---\n",
        "    # Ordenar todas las cajas directamente por su coordenada X para ir de izquierda a derecha.\n",
        "    # No se agrupa por líneas, lo que cumple con \"sin importar la altura\".\n",
        "    if boxes:\n",
        "        boxes.sort(key=lambda b: b[\"x_coord\"])\n",
        "        sorted_boxes = boxes # Todas las cajas ya están ordenadas por X\n",
        "    else:\n",
        "        sorted_boxes = [] # No hay cajas para procesar\n",
        "\n",
        "    # Procesar cada caja con el modelo OCR\n",
        "    for box_info in sorted_boxes:\n",
        "        x, y, w, h = box_info[\"box\"]\n",
        "\n",
        "        # Dibujar el bounding box en la imagen para visualización\n",
        "        cv2.rectangle(display_image, (x, y), (x + w, y + h), (0, 255, 0), 2) # Verde\n",
        "\n",
        "        # Recortar la región de interés (ROI)\n",
        "        roi = gray_image[y:y+h, x:x+w]\n",
        "\n",
        "        # --- Convertir cada BOX a negro y blanco (carácter blanco, fondo negro) ---\n",
        "        _, roi_binary = cv2.threshold(roi, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "        # -----------------------------------------------------------------------\n",
        "\n",
        "        # Preprocesar la ROI binarizada para la entrada del modelo\n",
        "        processed_roi = preprocess_image(roi_binary, target_size=(IMAGEN_ANCHO, IMAGEN_ALTO))\n",
        "\n",
        "        # Realizar la predicción\n",
        "        # El modelo espera un batch, así que expandimos las dimensiones: (1, alto, ancho, 1)\n",
        "        predictions = model.predict(np.expand_dims(processed_roi, axis=0), verbose=0)\n",
        "\n",
        "        # Obtener el índice de la clase predicha con mayor probabilidad\n",
        "        predicted_class_index = np.argmax(predictions[0])\n",
        "\n",
        "        # Convertir el índice a carácter\n",
        "        predicted_char = INDEX_TO_CHAR.get(predicted_class_index, '?') # Usar '?' para desconocidos\n",
        "\n",
        "        detected_characters.append(predicted_char)\n",
        "\n",
        "    # Unir todos los caracteres detectados en un solo string\n",
        "    result_string = \"\".join(detected_characters)\n",
        "\n",
        "    # Convertir la imagen de visualización de BGR a RGB para Gradio\n",
        "    display_image_rgb = cv2.cvtColor(display_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return result_string, display_image_rgb\n",
        "\n",
        "# --- Configuración de la interfaz Gradio ---\n",
        "iface = gr.Interface(\n",
        "    fn=run_ocr_inference,\n",
        "    inputs=gr.Image(type=\"numpy\", label=\"Sube una imagen para OCR\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Texto Reconocido\"),\n",
        "        gr.Image(type=\"numpy\", label=\"Imagen con Cajas Detectadas\")\n",
        "    ],\n",
        "    title=\"OCR Personalizado (A-Z, 0-9)\",\n",
        "    description=\"Sube una imagen con caracteres (letras mayúsculas y números) para que el modelo los detecte y reconozca.\"\n",
        ")\n",
        "\n",
        "# Lanzar la interfaz\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "brQlUmUp5xL6",
        "outputId": "5369627f-b1fc-4c72-b02e-3018e3e8a1c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://810e9d4b3d47582a39.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://810e9d4b3d47582a39.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}